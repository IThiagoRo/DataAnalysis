---
header-includes:
- \usepackage{longtable}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}\decimalpoint
- \setlength{\parindent}{1.25cm}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{float}
- \usepackage{multirow}
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: es
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =FALSE, message = FALSE, fig.align = "center", fig.height = 3.5, fig.pos = "H")

#librerias
library(tidyverse)
library(magrittr)
library(kableExtra)
library(xtable)
library(corrplot)
library(hrbrthemes)
library(ggplot2)
library(GGally)



#Selection of variables procedures 
library(olsrr)

#naive bayes
library(naivebayes)
library(e1071)
#knn
library(MASS)
library(class)
library(dummies)

#curva roc y modelo lda
library(ISLR)
library(MASS)
library(Amelia)
library(pscl)
library(caret)
library(pROC)
library(ROCR)
library(wesanderson)
```


```{=tex}
\input{DocumentFormat/titlepage.tex}
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
\listoffigures
\newpage
```

```{=tex}
\pagestyle{myheadings}
\setcounter{page}{4}
```

\section{Ejercicio1}
<!-- Daniel Hoyos -->



```{r db-ej1}
bank<-read.csv("Data/bank.csv", stringsAsFactors=TRUE, header = TRUE, sep = ",")
attach(bank)
```


\subsection{Análisis descriptivo de la base de datos bank}

La base de datos bank, contiene un total de 17 variables y 11.162 observaciones, de las cuales 7 son de tipo numéricas y 10 son de tipo categórica, esta base de datos resume algunas características acerca de clientes de un banco en particular tales como la edad(age), el tipo de trabajo que desempeña(job), el estado marital(marital), nivel educativo(education),si ha cometido o no alguna falta pagos(default), el estado de sus fondos económicos(balance), si el cliente tiene o no algún préstamo de vivienda(housing), si el cliente tiene o no algún préstamo (loan), medio de contacto con el cliente(contact),fecha de afiliación(day, month),tiempo de vencimiento (El tiempo de vencimiento). entre otras, a continuación veremos un pequeño resumen de las variables. 

```{r, fig.align='center', fig.width=70, fig.height=80}
kable(summary(bank[1:5]))
kable(summary(bank[6:11]))
kable(summary(bank[12:17]))
```

```{r, warning=FALSE, message=FALSE}
base<-bank[,c(1,6,10,12,13)]


gg2<-ggpairs(base,
             upper=list(continuous = wrap("smooth",alpha = 0.3, size=1.2,
                                               method = "lm")),
             lower=list(continuous ="cor"))

for(i in 1:ncol(base)){
  gg2[i,i]<-gg2[i,i]+
    geom_histogram(breaks=hist(base[,i],breaks = "FD",plot=F)$breaks,
                   
                   colour = "red",fill="lightgoldenrod1")
  
}

gg2
```




El gráfico anterior es importante para identificar el posible comportamiento de nuestras variables numéricas, en este caso vemos que existe poca relación lineal entre las mismas, situación que nos da una idea de pensar que es poco probable que existan problemas de multicolinealidad.


```{r , include=T,echo=F}
par(mfrow=c(2,3))
plot(factor(loan),age,xlab='loan',ylab='age',main='loan vs age')
plot(factor(loan),day,xlab='loan',ylab='day',main='loan vs day')
plot(factor(loan),duration,xlab='loan',ylab='duration',main='loan vs duration')
plot(factor(loan),balance,xlab='loan',ylab='balance',main='loan vs balance')
plot(factor(loan),xlab='loan',main='loan ')
```


La variable loan, es una de las variables de mayor interés en el estudio, es una variable dicótoma que representa si un cliente tiene o no algún préstamo, del gráfico anterior, claramente no hay diferencia en mediana para la variable loan con respecto a  la edad, día ,duración y balance, por otra parte, vemos que la mayoría de personas no tienen ningún tipo de préstamo.


## a) cree un conjunto de datos de entrenamiento del 75% y el restante 25% tratetelo como datos de test o prueba.


```{r, echo=TRUE}
df=data.frame(bank)
smp_sz <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_sz)

train <- df[train_ind, ] #datos de Entrenamiento 75%
test <- df[-train_ind, ] #datos de Test 25%

y_train=df[train_ind,8]
y_test=df[-train_ind,8]
```

\subsection{b)Con los datos de entrenamiento implemente naive bayes usando loan como el supervisor y las demas como pedictores.}

El clasificador Naive de Bayes asume que todas las features (componentes del vector x) son independientes y que son igualmente importantes. Con este supuesto, la probabilidad (Likelihood-verosilimitud) de observar el vector de features x = (x1, x2, . . . , xp) con p =1...16 dado que se esta en la clase j es:


$$Pr(x|Y=j)=Pr(x_1|Y=j)*...*Pr(x_p|Y=j)$$


Por el teorema de bayes tenemos:


$$Pr(y=j|x)=\frac{Pr(x|Y=j)(Pr(Y=j))}{Pr(x)}$$


lo anterior representa un modelo bayesiando donde, la distribucion posterior, esta representada por el producto de la verosimilitud y la probabilidad a priori.


ahora, vamos a implementar un modelo de naivebayes para calcular la probabilidad de que los proximos clientes tengan o no un credito.

 

```{r, echo=TRUE}
#modelo naive bayes
naiveB.fit <- naiveBayes(loan~., data=train,laplace=0.128)

#predict train and testing
predict_test<-predict(naiveB.fit,test,type="class")
predict_train<-predict(naiveB.fit,train,type="class")

#prediccion probabilidades
predict_train2<-predict(naiveB.fit,train,type="raw")
predict_test2<-predict(naiveB.fit,test,type="raw")
```

\subsection{c) Con los datos de entrenamiento, Implemente un modelo Knn con loan como supervisor y las demás como predictoras. utilizar varios valores de k, pero reportar solo uno.}

El modelo knn también conocido como k vecinos más cercanos, es una metodología muy eficiente que permite a partir de un valor de k, tomar los k valores más cercanos de una estimación para ajustarse a su comportamiento, a medida que incrementamos el valor de k, se tiende a perder la señal y comenzamos a guiarnos por el ruido del modelo, por lo tanto, se debe tener cuidado a la hora de utilizar esta metodología. 
por otra parte, en este modelo, se deben crear vectores de variables dummy para las variables categóricas, y las variables continuas se deben normalizar. cómo se verá a continuación.



```{r, echo = TRUE}
#modelo KNN
normalize <- function(x) {
norm <- ((x - min(x))/(max(x) - min(x)))
return (norm)
}
#variables categoricas, a vectores de dummy.
suppressWarnings(dummyjob<-dummy(df$job, sep="_"))
suppressWarnings(dummyMarital<-dummy(df$marital, sep="_"))
suppressWarnings(dummyEducation<-dummy(df$education, sep="_"))
suppressWarnings(dummydefault<-dummy(df$default ,sep="_"))
suppressWarnings(dummyhousing<-dummy(df$housing, sep="_"))
suppressWarnings(dummycontact<-dummy(df$contact ,sep="_"))
suppressWarnings(dummymonth<-dummy(df$month, sep="_"))
suppressWarnings(dummypoutcome<-dummy(df$poutcome, sep="_"))
suppressWarnings(dummydeposit<-dummy(df$deposit, sep="_"))

#normalización de las variables numericas.
age<-normalize(df$age)
balance<-normalize(df$balance)
day<-normalize(df$day)
duration<-normalize(df$duration)
campaign<-normalize(df$campaign)
pdays<-normalize(df$pdays)
previous<-normalize(df$previous)

#nueva base de datos, con las variables transformadas.
Newdata<-cbind(df,dummyjob,dummyMarital,dummyEducation,
               dummydefault,dummyhousing,dummycontact,dummymonth,
              dummypoutcome,dummydeposit,age,balance,day,duration,
              campaign,pdays,previous)

#nueva selección de datos de entrenamiento y test
train1 <- Newdata[ train_ind,18:50 ]
test1 <- Newdata[-train_ind,18:50 ]
y_train1=df[train_ind,8]
y_test1=df[-train_ind,8]

#Modelo 1 k=1
#fit.knn_train<-knn(train=train1, test=train1,cl=y_train1, k=1, prob=TRUE)
#fit.knn_Test<-knn(train=train1, test=test1,cl=y_train1, k=1, prob=TRUE)

#Modelo 2 k=2 modelo seleccionado
fit.knn_train<-knn(train=train1, test=train1,cl=y_train1, 
                   k=2, prob=TRUE,use.all=TRUE)

fit.knn_Test<-knn(train=train1, test=test1,cl=y_train1, k=2, prob=TRUE)

#modelo 3 k= 5
#fit.knn_train<-knn(train=train1, test=train1,cl=y_train1, 
#k=5, prob=TRUE,use.all=TRUE)
#fit.knn_Test<-knn(train=train1, test=test1,cl=y_train1, k=5, prob=TRUE)

#predict para verificar el ajuste de nuestros datos de Test.
Predicted_train<-factor(fit.knn_train)
Predicted_test<-factor(fit.knn_Test)

#probabilidades
prob_train <- attr(fit.knn_train, "prob")
prob_test <- attr(fit.knn_Test, "prob")
```

\subsection{d)Con los datos de entrenamiento, implemente un modelo Logístico con loan como supervisor y las demás como predictoras.}

### Planteamiento del Modelo.

$$Logit(\pi_i)=\log(\frac{\pi_i}{1-\pi_i})= \beta_0 + \beta_{1xi1}+ \beta_{2xi2}+...+\beta_{kxik}$$

El modelo logístico, es una variación del modelo de regresión lineal en el que la variable respuesta es una variable dicótoma, es decir solo toma 2 valores 0 ó 1, es por esto que el logit, o el logaritmo de la razón de dos se utiliza como su predictor lineal.
este modelo es naturalmente un modelo de clasificación, por lo anterior, el resultado que se va obtener es la probabilidad asociada a si una persona con diversas características tiene o no un crédito. para mayor comprensión, el modelo ajustado entregara el resultado de evaluar la siguiente expresión:


$$\pi_i= \frac{e^{\beta_0+\beta_{1xi1}+\beta_{2xi2}+...+\beta_{kxik}}}{1+e^{\beta_0+\beta_{1xi1}+\beta_{2xi2}+...+\beta_{kxik}}}$$

Si el valor de la probabilidad es mayor a 0.5 entonces esta persona tiene un crédito(loan=si), en caso contrario, no tiene crédito(loan=no).

```{r, echo = TRUE}
#Modelo logístico.
lr.fit=glm(as.factor(loan)~., data = train, family=binomial)

#predict para verificar ajuste.
lrPred_train<-predict(lr.fit,train, type = c("response"))
lrPred_test<-predict(lr.fit,test, type = c("response"))

#clasificación 
predict_lr_train<-ifelse(lrPred_train<=0.5,0,1)
predict_lr_test<-ifelse(lrPred_test<=0.5,0,1)
```

\subsection{e) Con los datos de entrenamiento, implemente un modelo LDA con loan como supervisor y las demás como predictoras. 
En LDA se modela la distribución de los predictores X de manera
separada en cada una de las categorías de respuesta (es decir, condicionando en Y ) y luego se usa el teorema de Bayes para obtener estimaciones de $Pr (Y = k | X = x)$ Cuando estas distribuciones se asumen normales, el modelo es muy similar en forma al de regresión logística. esta probabilidad de clasificaci;on esta dada por:}

$$Pr (Y = k | X = x)=\frac{\pi_kf_k(x)}{\sum_{l=1}^{k}\pi_lf_l(x)}$$

```{r, echo = TRUE}
#modelo LDA
lda.fit <- lda(as.factor(loan) ~., data=train)

#prediccion
predict_train_lda<-predict(lda.fit,train,type=c("class"))
predict_test_lda<-predict(lda.fit,test,type=c("class"))

#clasificación
train_lda<-ifelse(as.factor(train$loan)==predict_train_lda$class,0,1)
test_lda<-ifelse(as.factor(test$loan)==predict_test_lda$class,0,1)
```

\subsection{f) Con los datos de entrenamiento calcular training MSE, matriz confusión y curva roc para cada uno de los modelos.}


```{r, }
#training

#Matriz de confusión y  NB error
t_NB<-table(predict_train,y_train)
Train_error_NB<-(t_NB[1,2]+t_NB[2,1])/(sum(t_NB))
sensitidad_NB<-t_NB[2,2]/(t_NB[2,2]+t_NB[1,2])
especificidad_NB<-t_NB[1,1]/(t_NB[1,1]+t_NB[2,1])

#Matriz de confusión y knn error
t_KNN<-table(Predicted_train,y_train)
Train_error_Knn<-(sum(t_KNN[1,2],t_KNN[2,1]))/(sum(t_KNN))
sensitidad_knn<-t_KNN[2,2]/(t_KNN[2,2]+t_KNN[1,2])
especificidad_knn<-t_KNN[1,1]/(t_KNN[1,1]+t_KNN[2,1])

#Matriz de confusión y logistic error
T_lr<-table(as.factor(train$loan),predict_lr_train)
lr_train_err<-(T_lr[1,2]+T_lr[2,1])/(sum(T_lr))
sensitidad_lr<-T_lr[2,2]/(T_lr[2,2]+T_lr[1,2])
especificidad_lr<-T_lr[1,1]/(T_lr[1,1]+T_lr[2,1])

#Matriz de confusión y lda error
t_Lda<-table(as.factor(train$loan),train_lda)
LDA_train_err<-(t_Lda[1,2]+t_Lda[2,1])/(sum(t_Lda))
sensitidad_lda<-t_Lda[2,2]/(t_Lda[2,2]+t_Lda[1,2])
especificidad_lda<-t_Lda[1,1]/(t_Lda[1,1]+t_Lda[2,1])

list(NaiveBayes=t_NB,Train_NaiveBayes_err=Train_error_NB,
     Knn=t_KNN,Train_error_Knn=Train_error_Knn,
     logistic_reg=T_lr,lr_train_err=lr_train_err,
     Lda=t_Lda,LDA_train_err=LDA_train_err)


list(sensitividad_NaiveBayes=sensitidad_NB,Especificidad_NaiveBayes=especificidad_NB,
     sensitividad_Knn=sensitidad_knn,Especificidad_Knn=especificidad_knn,
    sensitividad_logistic_reg=sensitidad_lr,Especificidad_lr=especificidad_lr,
    sensitividad_Lda=sensitidad_lda,Especificidad_LDA=especificidad_lda)
     
```


```{r, }
#curvas Roc testing

par(mfrow=c(2,2))
#roc NB
plot(roc(as.factor(train$loan),1-predict_train2[,1],direction="<"),
     col="purple2", lwd=3, main="ROC curve NaiveBayes train",print.auc=T,print.thres = "best")

#roc knn
plot(roc(as.factor(train$loan),1-prob_train,direction="<"),
     col="purple2", lwd=3, main="ROC curve Knn k=2 train",print.auc=T,print.thres = "best")

#roc LR
plot(roc(as.factor(train$loan),lrPred_train,direction="<"),
     col="purple2", lwd=3, main="ROC curve Logistic regresion train",print.auc=T,print.thres = "best")

#roc LDA
plot(roc(as.factor(train$loan),1-predict_train_lda$posterior[,1],direction="<"),
     col="purple2", lwd=3, main="ROC curve LDA train",print.auc=T,print.thres = "best")

```


\subsection{g) Con los datos de test calcular training MSE, matriz confusión y curva roc para cada uno de los modelos.}

```{r, }
#testing

#Matriz de confusión y  NB error
t1_NB<-table(predict_test,y_test)
Test_error_NB<-(t1_NB[1,2]+t1_NB[2,1])/(sum(t1_NB))

#Matriz de confusión y knn error
t1_KNN<-table(Predicted_test,y_test1)
Test_error_Knn<-(sum(t1_KNN[1,2],t1_KNN[2,1]))/(sum(t1_KNN))

#Matriz de confusión y logistic error
T1_lr<-table(as.factor(test$loan),predict_lr_test)
lr_test_err<-(T1_lr[1,2]+T1_lr[2,1])/(sum(T1_lr))

#Matriz de confusión y lda error
t1_Lda<-table(as.factor(test$loan),test_lda)
LDA_test_err<-(t1_Lda[1,2]+t1_Lda[2,1])/(sum(t1_Lda))
sensitidad_ldatest<-t1_Lda[2,2]/(t1_Lda[2,2]+t1_Lda[1,2])
especificidad_ldatest<-t1_Lda[1,1]/(t1_Lda[1,1]+t1_Lda[2,1])


list(NaiveBayes=t1_NB,Test_error_NB=Test_error_NB,
     Knn=t1_KNN,Test_error_Knn=Test_error_Knn,
     logistic_reg=T1_lr,lr_test_err=lr_test_err,
     Lda=t1_Lda,LDA_test_err=LDA_test_err)
```

```{r , include=T,echo=FALSE}
#curvas Roc testing

par(mfrow=c(2,2))
#roc NB
plot(roc(as.factor(test$loan),1-predict_test2[,1],direction="<"),
     col="purple2", lwd=3, main="ROC curve NaiveBayes test",print.auc=T,print.thres = "best")

#roc knn
plot(roc(as.factor(test$loan),1-prob_test,direction="<"),
     col="purple2", lwd=3, main="ROC curve Knn k=2 test",print.auc=T,print.thres = "best")

#roc LR
plot(roc(as.factor(test$loan),lrPred_test,direction="<"),
     col="purple2", lwd=3, main="ROC curve Logistic regresion test",print.auc=T,print.thres = "best")

#roc LDA
plot(roc(as.factor(test$loan),1-predict_test_lda$posterior[,1],direction="<"),
     col="purple2", lwd=3, main="ROC curve LDA test",print.auc=T,print.thres = "best")

```

\subsection{h) Con cual modelo observo mejor desempeño y por qué?}
Para seleccionar el mejor modelo, es muy importante tener claro que es lo que se quiere responder. La variable loan como se dijo anteriormente representa si una persona tiene o no un crédito, esto es importante porque cometer un error de darle un crédito a una persona ya tenía uno o no darle un crédito a una persona que no lo tenía, es una decisión que genera un gran impacto negativo en las ganancias del banco, por lo cual, el modelo seleccionado debe cumplir con unos altos índices de sensitividad y de especificidad.
de lo anterior, sin duda alguna el modelo LDA, obtuvo unos índices de sensibilidad y de especificad superiores al 96%, lo cual es una excelente tasa de clasificación, por esta razón se selecciona como el mejor modelo, a pesar de que el AUC de las curvas Roc sean similares para todos los demás modelos.

```{r clean-env}
rm(list = ls())
```



\section{Ejercicio2}
<!-- Santiago Rojas -->

\subsection{Breve analisis Exploratorio} 

La base de datos costumer_loan_details, cuenta con un 12 variables y 114 observaciones de las cuales 4 son variables de tipo numérico y 8 son variables de tipo categóricas. Dicha base de datos tiene características de los cliente como lo son: state, gender, race, marital_status, occupation, credit_score, income, debts, loan_type, loan_decision_type.  

```{r, fig.align='center', fig.width=80}
data <- read.csv("Data/costumer_loan_details.csv",  
                 stringsAsFactors=TRUE, header = TRUE, sep = ",")
kable(summary(data[2:5]))
```
```{r, fig.align='center', fig.width=80}
kable(summary(data[6:10]))
```
```{r,fig.align='center', fig.width=80}
kable(summary(data[11:12]))
```



Del resumen numérico anterior se tiene: 

* El **state** con mayor numero de observaciones es **Other= 68** y el segundo mayor es **OH = 11**.

* El **gender** con mayor numero de observaciones es **Male = 91** y el gender con menor observaciones es **Female = 91**.

* El *age** promedio es de **39 años**.

* El *race** con mayor numero de observaciones es **American Indian or Alaska Native = 26**.

* El **marital_status** con mayor numero de observaciones es **Married = 44** y el siguiente con mayor numero de observaciones es **Single = 43**. 

* La **ocupation** con mayor numero de observaciones es **NYPD = 28** y el siquiente con mayor numero de observaciones es **IT = 27**. 

* El **credit_score** promedio es de **695.8**. 

* El **income** promedio es de **9338**. 

* El **debts** promedio es de **2744**. 

* El **loan_type** con mayor numero de observaciones es **Auto = 40** y el siguiente con mayor numero de observaciones es **Personal = 34**. 

* El **loan_decision_type** con mayor numero de observaciones es **Approved = 70 ** y el siguiente con mayor numero de observaciones es **Denied = 32**. 

```{r, message=FALSE, warning=FALSE}
base<-data[,c(4,8,10, 9)]


gg2<-ggpairs(base,
             upper=list(continuous = wrap("smooth",alpha = 0.3, size=1.2,
                                               method = "lm")),
             lower=list(continuous ="cor"))

for(i in 1:ncol(base)){
  gg2[i,i]<-gg2[i,i]+
    geom_histogram(breaks=hist(base[,i],breaks = "FD",plot=F)$breaks,
                   
                   colour = "red",fill="lightgoldenrod1")
  
}

gg2
```

Este gráfico nos permite mirar y evidenciar relaciones lineales entre las variables. 

Se puede observar relaciones lineales de interés entre las variables como los son: 

* Se observa una alta correlación  entre las variables **income** y **age** 
igual a **0.838**.

* Se observa una alta correlación  entre las variables **income** y **debts** 
igual a **0.818**.

* Se observa una moderada correlación entre las variables **age** y **debts** 
igual a **0.762**.

* Se observa una baja correlación entre las variables **age** y **credit_score** 
igual a **0.142**. 


## a) Cree un conjunto de datos de entrenamiento del 75% y el restante 25% trátelo como datos de test o de prueba. 

Se fija una **semilla = 123** con el fin de permitir replicabilidad del trabajo. Luego procedemos a crear el conjunto
de datos para entrenamiento **train** de un valor del **75%** de los datos para un total de **85 observaciones** y el 
conjunto de datos para prueba **test** de un valor del **25%** de los datos para un total de **29 observaciones**. 
Se define **income** como el supervisor **"Y"** y  la característica **debts** como predictor. 

```{r db-ej2, echo=TRUE, message=FALSE}
library(caret)
data <- read.csv("Data/costumer_loan_details.csv",  
                 stringsAsFactors=TRUE, header = TRUE, sep = ",")

set.seed(123) 



smp_sz <- floor(0.75 * nrow(data))
train_indx <- sample(seq_len(nrow(data)), size = smp_sz)

train <- data[train_indx, 10]
train_scale <- scale(data[train_indx, 10])
test <- data[-train_indx,10]
test_scale  <- scale(data[-train_indx,10])

y_train <- data[train_indx, 9] 
y_test <- data[-train_indx, 9] 

```

\subsection{b)Con los datos de entrenamiento, implemente Knn(con al menos tres valores para k) usando income como el supervisor y debts como predictor. Grafique e interprete.}

Con la ayuda de la libreria **caret** se realiza una regresión knn con un parametro de k = 16. Mas adelante se explica por que se obta por usar ese k. 

```{r knn-ej2, echo=TRUE}
knnmodel = knnreg(train_scale, y_train, k = 16)

pred_y = predict(knnmodel, data.frame(test_scale))

mse = mean((y_test - pred_y)^2)
mae = caret::MAE(y_test, pred_y)
rmse = caret::RMSE(y_test, pred_y)

#cat("MSE: ", mse, "MAE: ", mae, " RMSE: ", rmse)
```

| MSE | MAE | RMSE |
| --- | --- | ---- |
|  4398259 | 1814.891 | 2097.203 | 


```{r}
r1 <- cbind(test, y_test, 1) 
r2 <- cbind(test, pred_y, 2)

new_df <- rbind(r1, r2) %>% as.data.frame()
new_df$V3 <- as.factor(new_df$V3)
colnames(new_df) <- c("test", "y", "Valor")
new_df <- new_df %>% mutate(., Valor = if_else(Valor == "1", "Real", "Predicho"))

ggplot(new_df, aes(test, y, shape=Valor, size = Valor, color = Valor)) + 
  geom_point(size = 4)+
  scale_colour_discrete("Valor")+
  xlab("Debts")+
  ylab("Income")+
  ggtitle("Datos de prueba (Test)")+
  theme_bw()
```



```{r}
ggplot(new_df, aes(test, y, color = Valor)) + 
  geom_line()+
  geom_point()+
  scale_colour_discrete("Valor")+
  xlab("Debts")+
  ylab("Income")+
  ggtitle("Datos de prueba (Test)")+
  theme_bw()
```
De la anteriores gráficas se puede observar que las prediciones dadas por nuestro modelo de regresión knn son relativamente buenas ya que el modelo trata de capturar la tendencia y no el ruido, si es comparado entre las predicciones que puede arrojar otros modelos de knn. 

```{r}
n_iterations =  50

ans = c()

for(i in 1:n_iterations){
  knnmodel = knnreg(train_scale, y_train, k = i)
  pred_y = predict(knnmodel, data.frame(test_scale))

  mse = mean((y_test - pred_y)^2)
  ans[i] <- mse
}

db <- data.frame(k=c(1:n_iterations), db=ans)

db %>%
  ggplot(aes(k,db)) + geom_line() +
  geom_vline(xintercept = 16, linetype = 'dashed') +
  theme_minimal() +
  scale_x_continuous(breaks = seq(0,n_iterations,2)) +
  labs(title = 'MSE para diferentes valores de k')+
  ylab("MSE")

```

Nuestro modelo de regresión knn fue entrenado con 85 observaciones de un total de 114 y se toma **k = 16** por que es el modelo con menor **MSE = 4398259**, **MAE = 1814.891** , **RMSE = 2097.203** y cumple con ser el más parsimonioso entre los modelos de k = 17 hasta k = 50. 

\subsection{c) Con los datos de entrenamiento, implemente regresión lineal simple usando income como el supervisor y debts como predictor. Grafique e interprete.}

#### Correlación entre el supervisor (Income) y el predictor (debts). Datos de entrenamiento

```{r}
train_df <- cbind(train, y_train) %>% as.data.frame()
colnames(train_df) <- c("debts", "income")


corrplot(cor(train_df), method="numb", type="upper")
```

Se observa una relación lineal **media-alta** entre el supervisor **income** y el predictor **debts**. 


#### Modelo lineal

 
 $$Y \approx \beta_0 + \beta_1X_i + \epsilon_i; \: \epsilon_i \sim N(0, \sigma^2)$$



```{r lm-ej2, echo = TRUE}
model <- lm(y_train ~ debts, data=train_df)
```

$$\hat{Y} \approx 4584.5801 + 1.7285X_i + \epsilon_i; \: \epsilon_i \sim N(0, \sigma^2)$$


#### Anova 


```{r, eval = FALSE}
summary(model)
```


```{r}
kable(xtable(summary(model)))
```

#### Pruebas de hipotesis: Significancia del modelo. 

$$H_0: \beta_i = 0 \: vs \: H_i: \beta_i = 0$$
Con una significancia de $\alpha = 0.05$ y observando los p-valores para $\beta_0$ y para $\beta_1$ hay evidencia suficiente para rechazar a $H_0$ esto quiere decir que el modelo es significativo. 


#### Pruebas de hipotesis: Supuesto de normalidad.

$$H_0:\ \varepsilon_i\sim\ \text{ Normal.  vs }\ H_1:\ \varepsilon_i\not\sim\text{ Normal}$$

```{r}
res <- residuals(model)
shapiro <- shapiro.test(res)
shapvalue <- ifelse(shapiro$p.value < 0.001, "P value < 0.001", 
                    paste("P value = ", round(shapiro$p.value, 4), sep = ""))
shapstat <- paste("W = ", round(shapiro$statistic, 4), sep = "")
q <- qqnorm(res, plot.it = FALSE)
qqnorm(res, main = "Normal Q-Q Plot of Residuals")
qqline(res, lty = 2, col = 2)
text(min(q$x, na.rm = TRUE), max(q$y, na.rm = TRUE)*0.95, pos = 4,
     'Shapiro-Wilk Test', col = "blue", font = 2)
text(min(q$x, na.rm = TRUE), max(q$y, na.rm = TRUE)*0.80, pos = 4,
     shapstat, col = "blue", font = 3)
text(min(q$x, na.rm = TRUE), max(q$y, na.rm = TRUE)*0.65, pos = 4,
     shapvalue, col = "blue", font = 3)

```

Como el patrón de los residuales sigue en su mayoría la línea roja que representa el ajuste de la distribución de los residuales a una distribución normal, se concluye que el supuesto de normalidad se cumple. Lo cual se ratifica en el resultado de la prueba de normalidad de Shapiro-Wilk con un valor-P mayor a 0.05, por lo cual no se rechaza $H_0$ y se concluye que los residuales se distribuyen normal.


#### Supuesto de varianza constante.

Se realizó una prueba gráfica comparando los residuales con los valores ajustados para analizar su distribución.

```{r, echo=FALSE, out.width="60%", fig.align='center'}
# Cálculo de residuales estudentizados y valores ajustados
res.stud <- round(residuals(model), 4)
yhat <- model$fitted.values
# Gráfico de Residuales estudentizados vs. Valores ajustados
plot(yhat, res.stud, xlab = "Valores Ajustados", ylab = "Residuales", main = "Residuales vs. Valores ajustados")
abline(h = 0, lty = 2, col = 2)
```
    
De la gráfica se observa que el patrón formado por la nube de puntos no se aleja mucho de un patrón rectangular. Lo que nos da un indicio de homocedasticidad de varianza. 




```{r}
train_df %>% ggplot(., aes(x=debts, y=income))+
  geom_point()+
  geom_smooth(method=lm , color="red", fill="#69b3a2", se=TRUE)+
  theme_classic()
  
```

Del gráfico anterior y de las pruebas realizada nos permite concluir que una regresión lineal puede ser un modelo adecuado para poder explicar **income** dado que un cliente tiene una determinadad caracteristica **debts**. 


\subsection{d) Use los respectivos ajuste de cada uno de los modelos anteriores y con el conjunto de prueba, calcule el test-MSE. Qué observa?}



```{r, echo = TRUE, eval = FALSE}
test_df <- cbind(test, y_test) %>% as.data.frame()
colnames(test_df) <- c("debts", "income")

#testMSE Regresion lineal
preds <- predict(model, test_df)
modelEval <- cbind(test_df$income, preds) %>% as.data.frame()
mse_ml <- mean((modelEval$V1 - modelEval$preds)^2)
mse_ml

#testMSE Regresion KNN
pred_y = predict(knnmodel, data.frame(test_scale))
mse_knn = mean((y_test - pred_y)ˆ2)
mse_knn
```


| testMSE Regresión Lineal | testMSE Regresión Knn |
| ------------------------ | --------------------- |
|     3224949              |     4398259           |


Se observa que el modelo con menor **test MSE** es el de la regresión lineal. Dicho modelo a parte de hacer una mejor predicción también permite hacer inferencia, se recomienda dicho modelo como el modelo mas adecuado comparado respecto a los modelos obtenidos usando el método de aprendizaje estadístico KNN. 

\subsection{e) Usando todos los datos y regresión lineal multiple seleccione un modelo usando forward, backward y stepwise}

#### Forward

```{r, fig.width=70}
ols_step_forward_aic(lm(income ~., data=data[, 2:12,]))
```

#### Backward

```{r, fig.width=80}
ols_step_backward_aic(lm(income ~., data=data[, 2:12,]))
```

#### Stepwise 

```{r, fig.width=70}
ols_step_both_aic(lm(income ~., data=data[, 2:12,]))
```

\subsection{f) Seleccione uno de los modelos del paso anterior y responda con argumentación la pregunta: Ajusta bien dicho modelo?}

Dado que los métodos de selección automáticos como: Forward y Stepwise nos indican que dos modelo plausibles para explicar **income** son el modelo con la covariable **marital_status** y el modelo con la covariable **debts** obtamos por seleccionar el modelo con la covariable **debts** ya que cuenta con una de las mejores metricas(AIC, R-sq, Adj.R-sq) dadas por el metodo de selección automático de variables y como vimos anteriormente es un modelo que cumple con los supuesto de un modelo lineal ademas de tener una mejor predicción que los modelos de regresión Knn visto en este trabajo. Lo que nos permite concluir que dicho modelo es adecuado y tiene un buen ajuste. 

\section{Punto 3}

Se utilizan las técnicas ___ridge___ y ___lasso___ para regularizar las bases de datos __DATOS_A__ y __DATOS_B__. Se busca saber __¿Cuáles variables aparentemente muestran no ser relevantes para explicar la variable aleatoria Y?__

```{r, warning=FALSE,message=FALSE}
# cargado de librerias
library(tidyverse) # punto 3
library(dbplyr)
library(ggplot2)
library(corrplot) 
require(olsrr) # punto 4
library(kableExtra)
require(leaps)
require(glmnet) # ridge y lasso
```


Primeramente, es menester cargar las bases de datos y descubrir la estructura y comportamiento de dichas bases:

### DATOS_A

Se procede a cargar y examinar la base de datos:

```{r}
# cargando la base DATOS_A
datos_a <- read.table(file = "Data/DATOS_A.txt", header = T, sep = )
head(datos_a) # cabecera de los datos
tail(datos_a) # cola de los datos
```

```{r}
str(datos_a) # estructura y tipo de variables presentes en la base
```

Se observa como la base de datos __datos_a__ no presenta valores *NA*, además, todas las variables presentes en ella son de tipo numérico, cuenta con 15 variables (Y, X1, X2, ..., X14) y 1767 observaciones.

### Análisis descriptivo

A manera descriptiva se realizará una matriz de correlación de los datos.

```{r}
#library(corrplot)

# create correlation matrix
corr.data <- cor(datos_a)
# correlation plot of quantitative variables
corrplot(corr.data, method = 'ellipse', order='AOE', type = 'upper')
```

Se observa como las covariables que tienen más correlación con la variable respuesta __Y__ son __X10__ y __X6__ con correlación positiva y negativa respectivamente.

Luego de haber realizado el análisis descriptivo y haber examinado la base de datos, se procede con las técnicas de regularización mediante regresión ___ridge___ y ___lasso___.

## Ridge

Los métodos de __subset__ emplean mínimos cuadrados ordinarios (OLS) para ajustar un modelo lineal que contiene únicamente un subconjunto de predictores. Otra alternativa, conocida como regularización o __shrinkage__, consiste en ajustar el modelo incluyendo todos los predictores pero aplicando una penalización que fuerce a que las estimaciones de los coeficientes de regresión tiendan a cero. Con esto se intenta evitar __overfitting__, reducir varianza, atenuar el efecto de la correlación entre predictores y minimizar la influencia en el modelo de los predictores menos relevantes. Por lo general, aplicando regularización se consigue modelos con mayor poder predictivo (generalización). En esta ocasión, se utilizan los métodos de regularización Ridge y Lasso.

La regularización Ridge penaliza la suma de los coeficientes elevados al cuadrado $\left ( \left\| \beta \right\|^2 = \sum_{j =1}^{p} |\beta_j|^2 \right )$, La cual tiene el efecto de reducir de forma proporcional el valor de todos los coeficientes del modelo pero sin que estos lleguen a cero. El grado de penalización está controlado por el hiperparámetro $\lambda$. Cuando $\lambda = 0$, la penalización es nula y el resultado es equivalente al de un modelo lineal por mínimos cuadrados ordinarios (OLS). A medida que $\lambda$ aumenta, mayor es la penalización y menor el valor de los predictores.

A continuación, se presenta la realización de una regresión ___Ridge___ para la base __datos_a__:

Primero se crea el modelo de regresión __ridge__:

```{r}
#require(glmnet) # instaler paquete

# se crea el modelo
x<-model.matrix(Y~.,datos_a)[,-1]
y<-datos_a$Y

gridz<-10^seq(-2,10, length=100)
ridge.mod<-glmnet(x,y,alpha=0, lambda=gridz)

dim(coef(ridge.mod))
```

```{r}
par(mfrow=c(1,2))

plot(ridge.mod, xvar="lambda", label=TRUE, 
     main="The std ridge reg coef")
plot(ridge.mod, xvar="norm", label=TRUE, 
     main="Ridge regr coef using L1 norm")
```

De los dos gráficos, se observa como en el de los coeficientes estandarizados los valores convergen a cero a un valor loglambda aproximadamente igual a siete. Mientras que, en el segundo gráfico de los coeficientes usando L1 norm, los valores de los coeficientes de la regresión ridge convergen cuando este valor L1 es 0.

Luego, con los resultados obtenidos anteriormente, se procede a realizar la validación cruzada para cada valor de $\lambda$ y así estimar el error de validación cruzada.

```{r}
cedula<-1
set.seed(cedula) 
train<-sample(1: nrow(x), nrow(x)/2)
test<- -train
y.test<-y[test]
cv.out<-cv.glmnet(x[train,],y[train],alpha=0)
cv.out
```

Después, se escoge el "mejor" $\lambda$, es decir, el que produzca el menor error.

```{r}
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam
log(bestlam)
```

En este caso, se obtuvo que $\lambda = 0.8085394$ es el "mejor", es decir, el que produce el menor error.

Y finalmente, se realizan las predicciones del modelo de regresión __ridge__ para los coeficientes de las variables predictoras.

```{r}
ridge.pred<-predict(ridge.mod, s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out<-glmnet (x,y,alpha=0)
# names(out)
# plot(out$lambda,out$a0)
predict(out,type="coefficients",s=bestlam)[1:15,]
```

## Lasso

La regularización Lasso penaliza la suma del valor absolutos de los coeficientes de regresión $\left ( \left\| \beta \right\| = \sum_{j =1}^{p} |\beta_j| \right )$. Lo cual tiene el efecto de forzar a que los coeficientes de los predictores tiendan a cero. Dado que un predictor con coeficiente de regresión cero no influye en el modelo, lasso consigue excluir los predictores menos relevantes. Al igual que en ridge, el grado de penalización está controlado por el hiperparámetro $\lambda$. Cuando $\lambda = 0$, el resultado es equivalente al de un modelo lineal por mínimos cuadrados ordinarios. A medida que $\lambda$ aumenta, mayor es la penalización y más predictores quedan excluidos.

Seguidamente, se presenta la realización de una regresión ___Lasso___ para la base __datos_a__:

Primero se crea el modelo de regresión __lasso__:

```{r}
# regresión lasso
lasso.mod<-glmnet(x,y,alpha=1, lambda=gridz)

dim(coef(lasso.mod))
```

Luego se grafican la regresión buscando observar el valor de lambda adecuado:

```{r}
par(mfrow=c(1,2))

suppressWarnings( plot(lasso.mod, xvar="lambda", label=TRUE))
suppressWarnings( plot(lasso.mod, xvar="norm", label=TRUE, 
     main="Lasso reg coef using L1 norm"))
```

De los dos gráficos, se observa como en el de los coeficientes estandarizados los valores convergen a cero a un valor loglambda entre 0 y 1. Mientras que, en el segundo gráfico de los coeficientes usando L1 norm, los valores de los coeficientes de la regresión lasso convergen cuando este valor L1 es 4 aproximadamente.

Luego, con los resultados obtenidos anteriormente, se procede a realizar la validación cruzada para cada valor de $\lambda$ y así estimar el error de validación cruzada.

```{r}
cedula<-123
set.seed(cedula) 
train<-sample(1: nrow(x), nrow(x)/2)
test<- -train
y.test<-y[test]
cv.out<-cv.glmnet(x[train,],y[train],alpha=1)

plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam
log(bestlam)
```

Se obtuvo que $\lambda = 0.2042918$ es el "mejor", es decir, el que produce el menor error.

Y finalmente, se realizan las predicciones del modelo de regresión lasso para los coeficientes de las variables predictoras.

```{r}
lasso.pred<-predict(lasso.mod, s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

out<-glmnet (x,y,alpha=1)
lasso.coef<-predict(out,type="coefficients",s=bestlam)[1:15,]
lasso.coef

```

```{r}
lasso.coef[lasso.coef!=0] # arroja los coeficientes lasso distintos de 0
```

Note que, en estos últimos resultados arrojados por el R, se muestra como a diferencia de la regresion ridge, lasso fuerza y los hace cero a los coeficientes que con este criterio son los que menos aportan a explicar la variable __Y__. Por lo cual, las variables que si se toman en cuenta por la regresión lasso son las variables __X1__, __X2__, __X3__, __X4__, __X6__, __X8__, __X10__ y __X14__.

## Conclusiones y respuesta a la pregunta planteada

- Se observa como usando la regresión ridge, esta nos arrojó un $\lambda = 0.80$ el cual es mejor para minimizar el RSS. Además, los coeficientes arrojados por este tipo de regresión después de haber aplicado validación cruzada muestra aparentemente que, las variables que muestran no ser relevantes para explicar la variable respuesta __Y__ son las variables __X9__, __X11__ y __X12__, ya que son las que presentan los valores de los coeficientes más cercanos a cero en comparación con las demás. 

- Se observa como usando la regresión lasso, esta nos arrojó un $\lambda = 0.20$ el cual es mejor para minimizar el RSS. Además, los coeficientes arrojados por este tipo de regresión después de haber aplicado validación cruzada muestra aparentemente que, las variables que muestran no ser relevantes para explicar la variable respuesta __Y__ son las variables que no se toman en cuenta por la regresión lasso en la estimación de coeficientes mediante Cv. Es decir, las variables __X5__, __X6__, __X7__, __X9__, __X11__, __X12__ y __X13__.

### DATOS_B

Se procede a cargar y examinar la base de datos:

```{r}
# cargando la base DATOS_B
datos_b <- read.table(file = "Data/DATOS_B.txt", header = T, sep = )
head(datos_b) # cabecera de los datos
tail(datos_b) # cola de los datos
```

```{r}
str(datos_b) # estructura y tipo de variables presentes en la base
```

Se observa como la base de datos __datos_b__ no presenta valores *NA*, además, tres de sus  variables son de tipo int (entero), aunque la variable __X2__ podría ser de tipo factor, ya que solo cuenta con valores de 0 y 1. Y la variable __X3__ es de tipo caracter, que cuenta con los 7 días de la semana. La base cuenta con 4 variables (Y, X1, X2, X3) y 728 observaciones.

```{r}
# convirtiendo las variables X2 y X3 a factor
datos_b$X2 <- as.factor(datos_b$X2)
datos_b$X3 <- as.factor(datos_b$X3)

str(datos_b) # estructura y tipo de variables presentes en la base
```

### Análisis descriptivo

Se presentan algunos análisis descriptivos que ayudan a dilucidar el comportamiento de las covaraibles X¨s con respecto a la variable respuesta Y.

```{r, fig.height= 3.8, fig.width= 7}
cor(datos_b$Y,datos_b$X1) # correlación entre Y y X1
plot(datos_b$X1,datos_b$Y, xlab = "X1", ylab = "Y", # gráfico de Y vs X1
     main = "Gráfico Y vs X1")
```

Se observa como la Corr(Y, X1) = 0.91, lo cual implica que existe alta correlación positiva entre las variables. Por otro lado, el gráfico entre estas variables, muestra como existe una relación cuadrática o cúbica entre estas dos variables.

```{r, fig.height= 3.8, fig.width= 6, echo=FALSE}
X2_t <- c(525, 203)
xx <- barplot(X2_t, legend.text = c("0","1"), xlab = "X2", ylab = "Frecuencia relativa", col = c("lightblue", "mistyrose"), main = "Barplot de la variable X2", ylim = c(0, 600))
text(x=xx, y= X2_t, pos=3, cex=0.9, col="dark blue",
     label= X2_t)
```

Del gráfico del barplot de la variable X1 con respecto a la variable Y, se observa la diferencia entre las frecuencias en los valores "0" y "1" de la varaible X2.

```{r, echo=FALSE,message=FALSE}
myPal <- c(
  rgb(0, 94, 255, maxColorValue = 255),  
  rgb(255, 0, 174, maxColorValue = 255),  
  rgb(255, 136, 0, maxColorValue = 255),  
  rgb(119, 255, 0, maxColorValue = 255),
  rgb(209, 255, 0, maxColorValue = 255),
  rgb(600, 255, 0, maxColorValue = 800),
  rgb(300, 255, 0, maxColorValue = 400))
```


```{r,fig.width=8, fig.height= 4.2, fig.width= 7, echo=FALSE}
dias <- factor(datos_b$X3, levels = c("LUN","MAR","MIE","JUE","VIE", "SAB", "DOM"))
boxplot(datos_b$Y~ dias,
        col = myPal, ylab = "Y", main = "Boxplot por días")
```

Se observa en el gráfico de boxplot por días, como todas las cajas correspondientes a los días de la semana se encuentran centradas alrededor del valor de 500 de la variable Y. Además, los días que presentan mayor variabilidad en sus cajas son los lunes, martes, miércoles y viernes.

Luego de haber realizado el análisis descriptivo y haber examinado la base de datos, se procede con las técnicas de regularización mediante regresión ___ridge___ y ___lasso___.

Recordar que, el objetivo principal de estas metodologias es reducir la varianza de los estimadores de los paramteros, y con ello, dar respuesta a la pregunta de interes.

## Ridge

Como se pudo notar ya se detallo anteriormente el uso y funcionamiento del metodo, asi que ahora solo se procde a mostrar los resultados arrojados en este caso para la base __datos_b__.

```{r}
#require(glmnet) # instaler paquete
x_b<-model.matrix(Y~.,datos_b)[,-1]
y_b<-datos_b$Y

gridz<-10^seq(-2,10, length=100)
ridge.mod_b<-glmnet(x_b,y_b,alpha=0, lambda=gridz)

dim(coef(ridge.mod_b))
```

```{r}
par(mfrow=c(1,2))

plot(ridge.mod_b, xvar="lambda", label=TRUE, 
     main="The std ridge reg coef")
plot(ridge.mod_b, xvar="norm", label=TRUE, 
     main="Ridge regr coef using L1 norm")
```


```{r}
ced<-123
set.seed(ced) 
train_b<-sample(1: nrow(x_b), nrow(x_b)/2)
test_b<- -train_b
yb.test<-y_b[test_b]
cv.out_b<-cv.glmnet(x[train_b,],y_b[train_b],alpha=0)
cv.out_b
```

```{r}
plot(cv.out_b)
bestlam_b<-cv.out_b$lambda.min
bestlam_b
log(bestlam_b)
```

```{r}
ridge.pred_b<-predict(ridge.mod_b, s=bestlam_b,newx=x_b[test_b,])
mean((ridge.pred_b-yb.test)^2)
out_b<-glmnet (x,y,alpha=0)
# names(out)
# plot(out$lambda,out$a0)
predict(out_b,type="coefficients",s=bestlam_b)[1:15,]
```

## Lasso

Como se pudo notar ya se detalló anteriormente el uso y funcionamiento del método, así que ahora solo se procede a mostrar los resultados arrojados en este caso para la base __datos_b__.

```{r}
# regresión lasso
lasso.mod<-glmnet(x,y,alpha=1, lambda=gridz)

dim(coef(lasso.mod))
```

Luego se grafican la regresión buscando observar el valor de lambda adecuado:

```{r}
par(mfrow=c(1,2))

suppressWarnings( plot(lasso.mod, xvar="lambda", label=TRUE))
suppressWarnings( plot(lasso.mod, xvar="norm", label=TRUE, 
     main="Lasso reg coef using L1 norm"))
```

De los dos gráficos, se observa como en el de los coeficientes estandarizados los valores convergen a cero a un valor loglambda entre 0 y 1. Mientras que, en el segundo gráfico de los coeficientes usando L1 norm, los valores de los coeficientes de la regresión lasso convergen cuando este valor L1 es 4 aproximadamente.

Luego, con los resultados obtenidos anteriormente, se procede a realizar la validación cruzada para cada valor de $\lambda$ y así estimar el error de validación cruzada.

```{r}
cedula<-123
set.seed(cedula) 
train<-sample(1: nrow(x), nrow(x)/2)
test<- -train
y.test<-y[test]
cv.out<-cv.glmnet(x[train,],y[train],alpha=1)

plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam
log(bestlam)
```

Se obtuvo que $\lambda = 0.2042918$ es el "mejor", es decir, el que produce el menor error.

Y finalmente, se realizan las predicciones del modelo de regresión lasso para los coeficientes de las variables predictoras.

```{r}
lasso.pred<-predict(lasso.mod, s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

out<-glmnet (x,y,alpha=1)
lasso.coef<-predict(out,type="coefficients",s=bestlam)[1:15,]
lasso.coef

```

```{r}
lasso.coef[lasso.coef!=0] # arroja los coeficientes lasso distintos de 0
```

Note que, en estos últimos resultados arrojados por el R, se muestra como a diferencia de la regresion ridge, lasso fuerza y los hace cero a los coeficientes que con este criterio son los que menos aportan a explicar la variable __Y__. Por lo cual, las variables que si se toman en cuenta por la regresión lasso son las variables __X1__, __X2__, __X3__, __X4__, __X6__, __X8__, __X10__ y __X14__.

## Conclusiones y respuesta a la pregunta planteada

- Se observa como usando la regresión ridge, esta nos arrojó un $\lambda = 0.80$ el cual es mejor para minimizar el RSS. Además, los coeficientes arrojados por este tipo de regresión después de haber aplicado validación cruzada muestra aparentemente que, las variables que muestran no ser relevantes para explicar la variable respuesta __Y__ son las variables __X9__, __X11__ y __X12__, ya que son las que presentan los valores de los coeficientes más cercanos a cero en comparación con las demás. 

- Se observa como usando la regresión lasso, esta nos arrojó un $\lambda = 0.20$ el cual es mejor para minimizar el RSS. Además, los coeficientes arrojados por este tipo de regresión después de haber aplicado validación cruzada muestra aparentemente que, las variables que muestran no ser relevantes para explicar la variable respuesta __Y__ son las variables que no se toman en cuenta por la regresión lasso en la estimación de coeficientes mediante Cv. Es decir, las variables __X5__, __X6__, __X7__, __X9__, __X11__, __X12__ y __X13__.

\section{Punto 4}

Utilizando los métodos de selección y validación cruzada, se realiza un análisis de la base de datos ***surgical*** del paquete ___olsrr___ donde se seleccionen las variables más importantes para explicar la variabilidad del tiempo de supervivencia.

Primero, es menester poner en contexto acerca de la base ***surgical***. Dicha base cuenta con información sobre la supervivencia de los pacientes que se someten a una operación de hígado. Está compuesta por 54 registros y 9 variables, las cuales se detallan a continuación:

- __bcs:__ puntaje de coagulación de la sangre.

- __pindex:__ índice de pronóstico.

- __enzyme_test:__ puntaje de la prueba de función enzimática.

- __liver_test:__ puntuación de la prueba de función hepática.

- __age:__ edad en años.

- __gender:__ variable indicadora de género (0 = masculino, 1 = femenino).

- __alc_mod:__ variable indicadora de antecedentes de consumo de alcohol (0 = ninguno, 1 = moderado).

- __alc_heavy:__ variable indicadora de antecedentes de consumo de alcohol fuerte (0 = Ninguno, 1 = Mucho).

- __y:__ tiempo de supervivencia.

se procede a examinar la base de datos:

```{r}
# require(olsrr) # paqute donde se encuentra la base surgical

# base de datos SRUGICAL
names(surgical) # nombres de las columnas
dim(surgical) # dimensión de la base
head(surgical) # cabecera de la base
tail(surgical) # cola de la base
str(surgical) # estructura y tipo de variables presentes en la base

# se convierten las variables gender, alc_mod y alc_heavy
surgical$gender <- as.factor(surgical$gender)
surgical$alc_mod <- as.factor(surgical$alc_mod)
surgical$alc_heavy <- as.factor(surgical$alc_heavy)
```

Note que la base cuenta con todas sus variables numéricas, aunque se sabe que las variables __gender__, __alc_mod__ y __alc_heavy__ son de tipo indicadoras y cualitativas, correspondientes con el género e antedecentes de consumo de alcohol respectivamente. La base no cuenta con valores *NA*.

Adicionalmente, se presenta una tabla con los datos que contiene la base ***surgical***.

```{r}
# tabla de la base surgical
kable(surgical, booktabs = TRUE,format = "latex",
caption = "Base Surgical",
col.names = c("bcs", "pindex", "enzyme_test", "liver_test", 
              "age", "gender", "alc_mod",   "alc_heavy",  "y")) %>%
kable_styling( latex_options = c("striped", "condensed","HOLD_position"),
position = "center",
full_width = FALSE)
```

### Análisis descriptivo

Como un análisis descriptiva se realiza una matriz de correlación de los datos.

```{r}
#library(corrplot)
datos_sur <- surgical[,c(1,2,3,4,5,9)]
# create correlation matrix
corr_sur <- cor(datos_sur)
# correlation plot of quantitative variables
corrplot(corr_sur, method = 'ellipse', order='AOE', type = 'upper')
```

Del gráfico de correlaciones, se observa como la variable __liver_test__ es la que presenta mayor correlación positiva con respecto a la variable __y__. Es decir que, la puntuación de la prueba de función hepática está muy relacionada de manera directamente proporcional con el tiempo de supervivencia de los pacientes que se someten a una operación de hígado.

Después de poner a punto los datos y haber analizado los mismos, se realiza el proceso de selección y validación cruzada usando los métodos de ___best subset___, ___backward___, ___forward___ y ___stepwise___. Además de aplicar la validación cruzada.

## "Best" Subset

Primero se procede a seleccionar el "mejor" subconjunto para estos datos:

```{r}
regfit.full<-regsubsets(y~., data= surgical, nvmax=NULL, method = "exhaustive") 
#nvmax: maximum size of subsets to examine

reg.summary<-summary(regfit.full)
reg.summary

with(summary(regfit.full), data.frame(rsq,adjr2, cp, rss, bic, outmat))

names(reg.summary)
```

Luego, se presentan los gráficos con las 4 medidas de referencia que sugieren el número de parameros a tomar.

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary$rss ,xlab=" Number of Variables",ylab=" RSS",type="l", xlim=c(0,20))
a1<-which.min(reg.summary$rss)
points (a1, reg.summary$rss[a1], col ="red",cex =2, pch =20)

plot(reg.summary$adjr2 ,xlab =" Number of Variables",ylab=" Adjusted RSq"
     ,type="l")
a2<-which.max (reg.summary$adjr2)
points (a2, reg.summary$adjr2[a2], col ="red",cex =2, pch =20)

```

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary$cp ,xlab =" Number of Variables", ylab="Cp", type="l")
a3<-which.min(reg.summary$cp)
points (a3, reg.summary$cp [a3], col ="red",cex =2, pch =20)
a4<-which.min(reg.summary$bic)
plot(reg.summary$bic ,xlab=" Number of Variables",ylab=" BIC",type="l")
points (a4, reg.summary$bic [a4], col =" red",cex =2, pch =20)
```

Se puede observar como, usando el método de "the best subset" se proponen las siguientes cantidades a tomar según los 4 estimadores estadísticos:

- __RSS:__ propone 8 paramétros. 
- __R2adj:__ propone 5 paramétros. 
- __Cp:__ propone 4 paramétros. 
- __bIC:__ propone 4 paramétros. 

Ahora, se presentan también estos 4 graficos que nos ayudarán a dilucidar los paramétros que mejor ayudan a explicar la variable __y__

```{r, fig.height= 3.8, fig.width= 7, echo=FALSE}
plot(regfit.full, scale ="r2", main = "R2")
plot(regfit.full, scale ="adjr2", main = "adjR2")
plot(regfit.full, scale ="Cp", main = "Cp")
plot(regfit.full, scale ="bic", main = "BIC")
```

Se observa como, usando estas 4 medidas y el método "the best subset", las variables que mejor ayudan a explicar la variable respuesta __y__ son __bcs__, __pindex__, __enzyme_test__ y __alc_heavy__.

## Forward

Primero se procede a seleccionar el "mejor" subconjunto para estos datos:

```{r}
regfit.fwd<-regsubsets(y~.,data=surgical,
                       nvmax=NULL, method = "forward")
reg.summary_fwd <-summary(regfit.fwd)
with(summary(regfit.fwd), data.frame(rsq,adjr2, cp, rss, bic, outmat))
```

Luego, se presentan los gráficos con las 4 medidas de referencia que sugieren el número de parameros a tomar.

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary_fwd$rss ,xlab=" Number of Variables",ylab=" RSS",type="l", xlim=c(0,20))
b1<-which.min(reg.summary_fwd$rss)
points (b1, reg.summary_fwd$rss[b1], col ="blue",cex =2, pch =20)
plot(reg.summary_fwd$adjr2 ,xlab =" Number of Variables",ylab=" Adjusted RSq"
     ,type="l")
b2<-which.max (reg.summary_fwd$adjr2)
points (b2, reg.summary_fwd$adjr2[b2], col ="blue",cex =2, pch =20)
```

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary_fwd$cp ,xlab =" Number of Variables", ylab="Cp", type="l")
b3<-which.min(reg.summary_fwd$cp)
points (b3, reg.summary_fwd$cp [b3], col ="blue",cex =2, pch =20)
b4<-which.min(reg.summary_fwd$bic)
plot(reg.summary_fwd$bic ,xlab=" Number of Variables",ylab=" BIC",type="l")
points (b4, reg.summary_fwd$bic [b4], col =" blue",cex =2, pch =20)
```

Se puede observar como, usando el método de __forward__ se proponen las siguientes cantidades a tomar según los 4 estimadores estadísticos:

- __RSS:__ propone 8 paramétros. 
- __R2adj:__ propone 5 paramétros. 
- __Cp:__ propone 5 paramétros. 
- __bIC:__ propone 5 paramétros. 

Ahora, se presentan también estos 4 graficos que nos ayudarán a dilucidar los paramétros que mejor ayudan a explicar la variable __y__

```{r, fig.height= 3.8, fig.width= 7, echo=FALSE}
plot(regfit.fwd, scale ="r2", main = "R2")
plot(regfit.fwd, scale ="adjr2", main = "adjR2")
plot(regfit.fwd, scale ="Cp", main = "Cp")
plot(regfit.fwd, scale ="bic", main = "BIC")
```

Se observa como, usando estas 4 medidas y el método __forward__, las variables que mejor ayudan a explicar la variable respuesta __y__ son __bcs__, __pindex__, __enzyme_test__, __liver_test__ y __alc_heavy__.

## Backward

Primero se procede a seleccionar el "mejor" subconjunto para estos datos:

```{r}
regfit.bwd<-regsubsets(y~.,data=surgical,
                       nvmax=19, method = "backward")
reg.summary_bwd <- summary(regfit.bwd)
#reg.summary_bwd
with(summary(regfit.bwd), data.frame(rsq,adjr2, cp, rss, bic, outmat))
```

Luego, se presentan los gráficos con las 4 medidas de referencia que sugieren el número de parameros a tomar.

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary_bwd$rss ,xlab=" Number of Variables",ylab=" RSS",type="l", xlim=c(0,20))
c1<-which.min(reg.summary_bwd$rss)
points (c1, reg.summary_bwd$rss[c1], col ="green",cex =2, pch =20)

plot(reg.summary_bwd$adjr2 ,xlab =" Number of Variables",ylab=" Adjusted RSq"
     ,type="l")
c2<-which.max (reg.summary_bwd$adjr2)
points (c2, reg.summary_bwd$adjr2[c2], col ="green",cex =2, pch =20)
```

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary_bwd$cp ,xlab =" Number of Variables", ylab="Cp", type="l")
c3<-which.min(reg.summary_bwd$cp)
points (c3, reg.summary_bwd$cp [c3], col ="green",cex =2, pch =20)
c4<-which.min(reg.summary_bwd$bic)
plot(reg.summary_bwd$bic ,xlab=" Number of Variables",ylab=" BIC",type="l")
points (c4, reg.summary_bwd$bic [c4], col =" green",cex =2, pch =20)
```

Se puede observar como, usando el método de __backward__ se proponen las siguientes cantidades a tomar según los 4 estimadores estadísticos:

- __RSS:__ propone 8 paramétros. 
- __R2adj:__ propone 5 paramétros. 
- __Cp:__ propone 4 paramétros. 
- __bIC:__ propone 4 paramétros. 

Ahora, se presentan también estos 4 graficos que nos ayudarán a dilucidar los paramétros que mejor ayudan a explicar la variable __y__

```{r, fig.height= 3.8, fig.width= 7, echo=FALSE}
plot(regfit.bwd, scale ="r2", main = "R2")
plot(regfit.bwd, scale ="adjr2", main = "adjR2")
plot(regfit.bwd, scale ="Cp", main = "Cp")
plot(regfit.bwd, scale ="bic", main = "BIC")
```

Se observa como, usando estas 4 medidas y el método __backward__, las variables que mejor ayudan a explicar la variable respuesta __y__ son __bcs__, __pindex__, __enzyme_test__ y __alc_heavy__.

## Stepwise

Primero se procede a seleccionar el "mejor" subconjunto para estos datos:

```{r}
regfit.stepw<-regsubsets(y~., data= surgical, nvmax=NULL, method = "seqrep") 
#nvmax: maximum size of subsets to examine

reg.summary_stw<-summary(regfit.stepw)
reg.summary_stw

with(summary(regfit.stepw), data.frame(rsq,adjr2, cp, rss, bic, outmat))
```

Luego, se presentan los gráficos con las 4 medidas de referencia que sugieren el número de parameros a tomar.

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary_stw$rss ,xlab=" Number of Variables",ylab=" RSS",type="l", xlim=c(0,20))
d1<-which.min(reg.summary_stw$rss)
points (d1, reg.summary_stw$rss[d1], col ="orange",cex =2, pch =20)

plot(reg.summary_stw$adjr2 ,xlab =" Number of Variables",ylab=" Adjusted RSq"
     ,type="l")
d2<-which.max (reg.summary_stw$adjr2)
points (d2, reg.summary_stw$adjr2[d2], col ="orange",cex =2, pch =20)
```

```{r, fig.height= 3.5, fig.width= 7, echo=FALSE}
par(mfrow =c(1,2))
plot(reg.summary_stw$cp ,xlab =" Number of Variables", ylab="Cp", type="l")
d3<-which.min(reg.summary_stw$cp)
points (d3, reg.summary_stw$cp [d3], col ="orange",cex =2, pch =20)
d4<-which.min(reg.summary_stw$bic)
plot(reg.summary_stw$bic ,xlab=" Number of Variables",ylab=" BIC",type="l")
points (d4, reg.summary_stw$bic [d4], col =" orange",cex =2, pch =20)
```

Se puede observar como, usando el método de __stepwise__ se proponen las siguientes cantidades a tomar según los 4 estimadores estadísticos:

- __RSS:__ propone 8 paramétros. 
- __R2adj:__ propone 8 paramétros. 
- __Cp:__ propone 8 paramétros. 
- __bIC:__ propone 8 paramétros. 

Ahora, se presentan también estos 4 graficos que nos ayudarán a dilucidar los paramétros que mejor ayudan a explicar la variable __y__.

```{r, fig.height= 3.8, fig.width= 7, echo=FALSE}
plot(regfit.stepw, scale ="r2", main = "R2")
plot(regfit.stepw, scale ="adjr2", main = "adjR2")
plot(regfit.stepw, scale ="Cp", main = "Cp")
plot(regfit.stepw, scale ="bic", main = "BIC")
```

Se observa como, usando estas 4 medidas y el método __stepwise__, considera que todas las variables son buenas para explicar a la varaible __y__.

## Validación cruzada (CV)

Primero se procede a preparar los datos:

```{r}
# generando la muestra de entrenamiento
set.seed (1)
train<-sample (c(TRUE ,FALSE), nrow(surgical),rep=TRUE)
test<-(!train )
regfit.best<-regsubsets(y~.,data=surgical[train,],
                        nvmax = NULL)

# Creando una función para evaluar los modelos:
predict.regsubsets =function (object,newdata,y){
  form<-as.formula(object$call[[2]])
  mat<-model.matrix(form ,newdata)
  val.errors =rep(NA, (ncol(mat)-1))
  for(i in 1:length(val.errors)){
    coefi<-coef(object ,id=i)
    xvars<-names (coefi)
    pred<-mat[,xvars]%*%coefi
    val.errors [i]= mean((y-pred)^2)
  }
  val.errors
}
```

Luego se realizan las predicciones.

```{r}
e1<-predict.regsubsets(regfit.best,surgical[test,],
                       surgical$y[test])

b1<-which.min(e1)

regfit.best<-regsubsets(y~.,data=surgical ,nvmax =NULL)

e1
b1
coef(regfit.best,8)
```

```{r, fig.height= 3.8, fig.width= 7, echo=FALSE}
plot(e1,type="b", col="purple", pch=20, cex=2)
```

Luego el gráfico nos muestra que, usando __CV__, un modelo que ayuda explicar de mejor manera la varaible respuesta __y__ es un que cuenta con 4 paramétros (sin contar el intercepto), y corresponde con las variables __bcs__, __pindex__, __enzyme_test__ y __alc_heavy__.



