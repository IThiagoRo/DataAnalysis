---
header-includes:
- \usepackage{longtable}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}\decimalpoint
- \setlength{\parindent}{1.25cm}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{float}
- \usepackage{multirow}
output:
  pdf_document: 
    number_sections: yes
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: "es"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", fig.height = 3.2, fig.pos = "H")

library(magrittr)
library(knitr)
library(tidyverse)
library(readxl)
```

```{=tex}
\input{DocumentFormat/titlepage.tex}
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
\listoffigures
\newpage
```

```{=tex}
\pagestyle{myheadings}
\setcounter{page}{4}
```

\section{Ejercicio1}
<!--Genaro -->

\section{Ejercicio2}
```{r}
db <- read_xlsx("Data/calibracion_cronometro.xlsx")

#Mirar cual es la variable x y cual es la variable y 
db %>% ggplot(., aes(x=y, y=x)) +
   geom_point()
```

\section{Ejercicio3}
<!--Santiago Rojas -->
\subsection{K-nearest neighbors (KNN)}

$$Pr(Y = J \mid X = x_0) \approx \frac{1}{K} \sum_{i \in N_0}I(y_i=j)$$

```{r}
X1 <- c(0, 2, 0, 0, -1, 1)
X2 <- c(3, 0, 1, 1,  0, 1)
X3 <- c(0, 0, 3, 2,  1, 1)
Y <- c("Red", "Red", "Red", "Green", "Green", "Red")

db <- cbind(X1,X2,X3,Y)
db <- as.data.frame(db)
db$Y <- as.factor(db$Y)
```


```{r}
rm(X1,X2,X3,Y)
kable(db, caption = "Base de datos")
```

\subsection{a) Distancia a cada observación}

Usando la distancia euclidiana entre dos punto $u$ y $v$ definida como: 

$$d(u, v) = \sqrt{(u_1- v_1)^2+(u_2-v_2)^2+(u_3-v_3)^2}$$
Calculamos la distancia entre cada observación y el punto $X_1 = X_2 = X_3 = 0$ usando R. 

```{r, echo = TRUE}
point <- c(0, 0, 0)

dist_eucl <- function(x){
  ans <- c()
  for (i in 1:nrow(x)){
    xi <- as.numeric(t(as.vector(x[i, ])))
    result <- sqrt(sum((xi-point)^2))
    ans <- append(ans, result)
  }
  return (ans)
}

db <- mutate(db, dist = dist_eucl(db[1:3]))
```
Se obtiene: 

```{r}
db %>% mutate(., obs = 1:6) %>% select(obs, Y, dist) %>%
  kable(., caption = "Distancia a cada observación desde el punto $X_1 = X_2 = X_3 = 0$", col.names = c("Observación", "Grupo", "Distancia Euclidiana"))
```


\subsection{b) Predicción para K = 1}

Con una selección de K = 1. Knn identifica la observación más cercana al punto con características $X_1 = X_2 = X_3 = 0$ y en este caso la observación mas cercana es la **numero 5** con una distancia de **1.414214**. Dando así Knn una  estimación de $1/1$ de pertenecer al grupo **Green**. Por ende la estimación es pertenecer a la clase **Green**. 

\subsection{c) Predicción para K = 3}
Con una selección de K = 3. Knn identifica las 3 observaciones más cercanas al punto con características $X_1 = X_2 = X_3 = 0$ y en este caso las observaciones mas cercana son la **numero 5**, la **numero 6** y la **numero 2** que consisten en 2 observaciones de la clase **Red** y una observación de la clase **Green**, dando como restulado una estimación de $2/3$ de pertenecer a la clase **Red** y $1/3$ de pertenecer a la clase **Green**. Por consiguiente se estima pertenecer a la clase **Red**.

\subsection{d) Frontera de decisión de Bayes}

Si la frontera de decisión de Bayes en este problema es altamente no
lineal, ¿esperaríamos que el mejor valor de K fuera grande o
pequeño? ¿Por qué?

\section{Ejercicio4}
<!--Daniel Hoyos --> 