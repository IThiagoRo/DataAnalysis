---
header-includes:
- \usepackage{longtable}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}\decimalpoint
- \setlength{\parindent}{1.25cm}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{float}
- \usepackage{multirow}
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: yes
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: es
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", fig.height = 3.5, fig.pos = "H")

library(magrittr)
library(tidyverse)
library(readxl)
library(kableExtra)
library(knitr)
library(corrplot)
```

```{=tex}
\input{DocumentFormat/titlepage.tex}
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
\listoffigures
\newpage
```

```{=tex}
\pagestyle{myheadings}
\setcounter{page}{4}
```

\section{Ejercicio1}
\subsection{Clasificador de Bayes un gold estándar}

El clasificador de Bayes produce la menor tasa de error de prueba. Demostración: 

Se tiene que la tasa de error de prueba está dada por:
$$Average(I(y_0 \neq \hat{y}_0))=E[I(y_0 \neq \hat{y}_0)]$$

Se supone que se está trabajando con dos clases (Aunque también se puede generalizar para $n$ clases). Donde se suponen los siguientes datos de prueba:

$$ y_{0i} = \left\lbrace \begin{array}{ll}
0 ~, ~ i=1,2,...,n;~Siendo~x_{0i}~los~datos~de~prueba \\
1
\end{array}
\right.$$

Y su respectivo clasificador de Bayes está dado por

$$P(y_{0i} = j |X_{0i}= x_{01}),~ con ~ j = 0,1$$

Donde $$\underbrace{P(y_{0i} = 0 | x_{0i})}_{P_0} ~ > ~ \acute{o} ~ < ~ \underbrace{P(y_{0i} = 1 |x_{0i})}_{P_1}$$

Se toma la máxima probabilidad de las dos probabilidades condicionales anteriores, es decir $máx\{P_0, P_1\}$

El error se minimiza cuando $máx\{P_0, P_1\}$ lleva a aciertos, el cual se reduce al minimizar la tasa de error promedio cuando se usa una indicadora (Clasificador):

La siguiente igualdad es un resultado de probabilidad (Solo se cumple para la indicadora):

$$\underbrace{E[I(\underbrace{y_{0i}\neq \widehat{y}_{0i}) }_{A})|x_{0i}]}_{Se~ desea~ minimizar}=P(y_{0i}\neq \widehat{y}_{0i}|x_{0i})=0*P(I_A=0)+1*(I_A=1)$$

 El menor valor que puede obtener es:

$E[I_{A}|x_{0i}]  = 0$ que sucede cuando $y_{0i}=\hat{y}_{0i}~~$ 
$\Longrightarrow~~$ $(y_{0i} - \hat{y}_{0i}) = 0$  $~~\Longrightarrow~~$
$(y_{0i} - \hat{y}_{0i})^2=0$

Seguidamente, se intentará probar que $y_{0i} = \hat{y_{0i}}$ produce el menor error Cuando se usa el método de Bayes.


Sea $f(y) = (y - \hat{y})^2$. Donde $E[f(y)] = E[(y - \hat{y})^2]$ es el $MSE$, que es mínimo justamente cuando $y_i$ es el promedio de los errores.


Posteriormente, se utiliza una variable binaria que indiqué cuando se comete o no un error:

$I\{y_{0i} \neq \hat{y}_{01}\}$ el cual toma valores de $\left\{
\begin{aligned}
0\\
1\\
\end{aligned}
\right.$
al igual que $y_{0i}$ y el clasificador $\hat{y}_{0i}$.

Entonces $(y_{0i} - \hat{y}_{0i})^2 = I\{y_{0i} \neq \hat{y}_{0i}\}$, esta igualdad se prueba a continuación:

sea:

$$y_{0i} = I\{y_{0i} = j\}~~ y ~~ \hat{y}_{0i} = I\{\hat{y}_{0i} = j\} \longrightarrow ~~ ambas~ son ~indicadoras.$$  

Utilizando la función de perdida 

$$\delta(x) = (y_{0i} - \hat{y}_{0i})^2 = [I\{y_{0i} = j\} - I\{\hat{y}_{0i} = j\}]^2$$

Se prueba que:

$$ \begin{array}{ c | c }
[I\{y_{0i} = j\} - I\{\hat{y}_{0i} = j\}]^2 & I\{y_{0i} \neq\ \hat{y}_{0i}\} \\ 
\hline
(1 - 1)^2 = 0 & 0 \\ 
(1 - 0)^2 = 1 & 1  \\ 
(0 - 1)^2 = 1 & 1  \\ 
(0 - 0)^2 = 0 & 0  \\
\end{array} $$

Así que $(y_{0i} - \hat{y}_{0i})^2$ es equivalente a $I\{y_{0i} \neq \hat{y}_{0i} \}$

Este proceso de clasificación esta basado en la regla de clasificación de Bayes por lo tanto minimiza $(y_{0i} - \hat{y}_{0i})^2$ y minimiza la tasa de error de prueba.

Esta expresión anterior se demostrará a continuación:


Se define la función de pérdida posterior como:

$$ L(y_{0i},\delta(x)) = (y_{0i}-\hat{y}_{0i})^2 = [I\{y_{0i}=j\}-I\{\hat{y}_{0i}=j\}]^2 $$
Consideremos $y = y_{0i}$, $x= x_{0i}$ para facilitar el proceso algebráico a continuación:

$$ L(y,\delta(x)) = (y-\hat{y})^2 = [I\{y=j\}-I\{\hat{y}=j\}]^2 $$

Sea la función de pérdida posterior esperada:


$$
\begin{aligned}
\gamma(y,\delta(x)) & = E[L(y,\delta(x))|X=x]\\
& = E[(y-\hat{y})^2|X=x]\\
& = E[(y-E[y|x]+E[y|x]-\hat{y})^2|X=x]\\
& = E[(y-E[y|x])^2 + (E[y|x]-\hat{y})^2 + 2(y-E[y|x])(E[y|x]-\hat{y})|X=x]\\
& = E[(y-E[y|x])^2|X=x]+(E[y|x]-\hat{y})^2 + 2 
\underbrace{E[(y-E[y|x])(E[y|x]-\hat{y})|X=x]}_{\blacksquare}
\end{aligned}
$$

$$
\begin{aligned}
\blacksquare E[(y-E[y|x])(E[y|x]-\hat{y})|X=x] & = E[y \cdot E[y|x] - y\cdot\hat{y}-E^2[y|x] + \hat{y} \cdot E[y|x]|X=x]\\
& = \color{blue}{E[y|x]E[y|x]} - \color{red}{\hat{y}E[y|x]}-\color{blue}{E^2[y|x]} + \color{red}{\hat{y}E[y|x]}\\
& = 0
\end{aligned}
$$

Por lo tanto

$$\gamma(y, \delta(x))  = E[(y-E[y|x])^2|X=x]+(E[y|x]-\hat{y})^2
\geq \underbrace{E[(y-E[y|x])^2|X=x]}_{Cota~inferior:~mínimo}$$


Por lo tanto el mínimo valor que toma $\gamma(y, \delta(x))$ es: 

$$E[(y-E[y|x])^2|X=x] ~ \text{que es cuando}~ (E[y|x]-\hat{y})^2 = 0$$

Dado que $E[(y-E[y|x])^2|X=x]$ es la cota mínima, se prueba que esta utiliza el estimador de Bayes para estimar $\hat{y}$ y lo esta utilizando mediante $E[y|x]: (E[(y- \color{purple}{E[y|x]})^2|X=x])$ donde:


$$\widehat{y}=E[y|x]=\underbrace{P(y|x)}_{Estimador ~ de~  Bayes}$$




\section{Ejercicio2}
\subsection{Analisis Descriptivo}
```{r}
db <- read_xlsx("Data/calibracion_cronometro.xlsx")
db <- db %>% select(x,y)
kable(summary(db), col.names = c("x", "Tiempo"))
```

```{r}
attach(db)
db %>% ggplot(., aes(x=y, y=x)) +
  geom_point()+
  ggtitle("Diagrama de dispersión")+
  xlab("Tiempo(seg)")
  
```
```{r, warning=FALSE}
attach(db)
db %>% ggplot(., aes(x=y, y=x))+ 
  geom_boxplot()+ 
  ggtitle("Box-plot ")+
  xlab("Tiempo(seg)")
  
```
**Matriz de Autocorrelación**

```{r}
db %>% as.matrix() %>% cor() %>% corrplot(., method="number")
```


\subsection{Modelo Parametrico}

**Modelo lineal** 

$$Y = \beta_0 + \beta_1x+\epsilon \: ; \: \epsilon \sim N(0, \sigma^2)$$


```{r}
model <- lm(db$y ~ db$x)
summary(model)
```


```{r, warning = FALSE, message=FALSE}
db %>% ggplot(., aes(x=y, y=x)) +
  geom_point() +
  geom_smooth(method=lm , color="red", se=FALSE)+
  ggtitle("Modelo Lineal")+
  xlab("Tiempo(seg)")
```

```{r}
data=data.frame(x,y)
attach(data)
library(lattice)
library(caret)
model = knnreg(x ~ y,data=data)
set.seed(1)
model <- train(
  x ~ y,
  data = data,
  method = 'knn'
)
model
plot(model)
```

```{r}
library(deming)
afit1 <- theilsen(x ~ y, symmetric=TRUE)
afit1
regresion <- lm(x ~ y)
plot(y,x,main = 'No parametric Regression Theil Model')
abline(afit1,col='blue')
abline(regresion,col='red')
```


\subsection{Modelo No Parametrico}

**Test de Spearmn**

El test de Spearman es la contra parte no parametrica del test de correlación de Pearson, ambos buscan encontrar y cuantificar el grado de relación lineal entre dos variables.
partiendo de lo anterior, como vimos en la sección anterior, las variables x e y presentan un muy buen ajuste lineal que deriva en una dependencia bastante marcada, el test de Spearman nos permitirá corroborar esta dependencia
y adicionalmente, nos permitiría calcular un valor para la correlación entre las mismas. como veremos a continuación. 

Prueba unilateral derecha:

* $H_0:$ X e Y son mutuamente independientes.
* $H_a:$ Existe una tendencia a formar parejas entre los valores grandes de X e Y.

Estadístico de prueba:
El test de Pearson, presenta dos tipos de estadísticos de prueba, uno cuando
existen repeticiones entre las observaciones, y otro en caso contrario, para el estudio que estamos llevando a cabo se verifico dicha situación, y nos encontramos con que efectivamente no tenemos ninguna observación repetida, por lo cual el siguiente es el estadístico de 

Prueba a utilizar:

$\rho = 1 - \frac{6T}{n(n^2-1)}$ donde $T = \sum_{i=1}^{n} [R(x_i)-R(y_i)]^2$

Criterio de rechazo: 
$Rc=[\rho/\rho <- W\alpha]$ donde $W\alpha$ valor tabulado en la tabla A.10 con la aproximación normal.
$VP = P(Z < \rho\sqrt{n-1})$ para un valor de $\alpha = 0.05$ se rechaza $H_0$ si el Valor p $< \alpha$ 


```{r}
cor.test(db$x, db$y,alternative="greater",method="s")
```


Para un valor de $\alpha = 0.05$ existe suficiente evidencia para rechazar $H_0$, por lo cual se concluye que existe una fuerte dependencia positiva entre X e y, con un valor de $\rho = 1$ que indica que es completamente lineal.

\section{Ejercicio3}
\subsection{K-nearest neighbors (KNN)}

$$Pr(Y = J \mid X = x_0) \approx \frac{1}{K} \sum_{i \in N_0}I(y_i=j)$$

```{r}
X1 <- c(0, 2, 0, 0, -1, 1)
X2 <- c(3, 0, 1, 1,  0, 1)
X3 <- c(0, 0, 3, 2,  1, 1)
Y <- c("Red", "Red", "Red", "Green", "Green", "Red")

db <- cbind(X1,X2,X3,Y)
db <- as.data.frame(db)
db$Y <- as.factor(db$Y)
```


```{r}
rm(X1,X2,X3,Y)
db %>% kable(., caption = "Base de datos") %>%
   kable_styling(latex_options = "HOLD_position")
```



\subsection{a) Distancia a cada observación}

Usando la distancia euclidiana entre dos punto $u$ y $v$ definida como: 

$$d(u, v) = \sqrt{(u_1- v_1)^2+(u_2-v_2)^2+(u_3-v_3)^2}$$
Calculamos la distancia entre cada observación y el punto $P$ con características $X_1 = X_2 = X_3 = 0$ 

* $d(p, x_1) = \sqrt{(p_1- x_{1,1})^2+(p_2-x_{1,2})^2+(p_3-x_{1,3})^2} = \sqrt{(0-0)^2 + (0-3)^2 +(0-0)^2} = \sqrt{0 + 9 + 0} = \sqrt{9} = 3$

* $d(p, x_2) = \sqrt{(p_1- x_{2,1})^2+(p_2-x_{2,2})^2+(p_3-x_{2,3})^2} = \sqrt{(0-2)^2 + (0-0)^2 + (0-0)^2} = \sqrt{4 + 0 + 0} = \sqrt{4} = 2$

* $d(p, x_3) = \sqrt{(p_1- x_{3,1})^2+(p_2-x_{3,2})^2+(p_3-x_{3,3})^2} = \sqrt{(0-0)^2 + (0-1)^2 + (0-3)^2} = \sqrt{0 + 1 + 9} = \sqrt{10} = 3.162278$

* $d(p, x_4) = \sqrt{(p_1- x_{4,1})^2+(p_2-x_{4,2})^2+(p_3-x_{4,3})^2} = \sqrt{(0-0)^2 + (0-1)^2 + (0-2)} = \sqrt{0 + 1 + 4} = \sqrt{5} = 2.236068$

* $d(p, x_5) = \sqrt{(p_1- x_{5,1})^2+(p_2-x_{5,2})^2+(p_3-x_{5,3})^2} = \sqrt{(0+1)^2 + (0-0)^2 + (0-1)^2} = \sqrt{1 + 0 + 1} = \sqrt{2} = 1.414214$

* $d(p, x_6) = \sqrt{(p_1- x_{6,1})^2+(p_2-x_{6,2})^2+(p_3-x_{6,3})^2} = \sqrt{(0-1)^2 + (0-1)^2 + (0-1)^3} = \sqrt{1 + 1 +1} = \sqrt{3} = 1.732051$


Ahora procedemos a calcularla con R:  

```{r, echo = TRUE}
point <- c(0, 0, 0)

dist_eucl <- function(x){
  ans <- c()
  for (i in 1:nrow(x)){
    xi <- as.numeric(t(as.vector(x[i, ])))
    result <- sqrt(sum((xi-point)^2))
    ans <- append(ans, result)
  }
  return (ans)
}

db <- mutate(db, dist = dist_eucl(db[1:3]))
```

```{r}
db %>% mutate(., obs = 1:6) %>% select(obs, Y, dist) %>%
  kable(., caption = "Distancia a cada observación desde el punto $X_1 = X_2 = X_3 = 0$", col.names = c("Observación", "Grupo", "Distancia Euclidiana")) %>%  kable_styling(latex_options = "HOLD_position")
```


\subsection{b) Predicción para K = 1}

Con una selección de K = 1. Knn identifica la observación más cercana al punto con características $X_1 = X_2 = X_3 = 0$ y en este caso la observación mas cercana es la **numero 5** con una distancia de **1.414214**. Dando así Knn una  estimación de $1/1$ de pertenecer al grupo **Green**. Por ende la estimación es pertenecer a la clase **Green**. 

Usando la librería **Class** y la funcion **knn()** se obtiene: 


```{r}
db <- db %>% select(X1, X2, X3, Y)
```


```{r, echo = TRUE}
library(class)
point <- c(0,0,0)
n <- 6 

model <- knn(train=db[, -4], test=point, cl=db[1:6, 4], k = 1)
kable(model, col.names = c("Predicción"))
```



\subsection{c) Predicción para K = 3}
Con una selección de K = 3. Knn identifica las 3 observaciones más cercanas al punto con características $X_1 = X_2 = X_3 = 0$ y en este caso las observaciones mas cercana son la **numero 5**, la **numero 6** y la **numero 2** que consisten en 2 observaciones de la clase **Red** y una observación de la clase **Green**, dando como restulado una estimación de $2/3$ de pertenecer a la clase **Red** y $1/3$ de pertenecer a la clase **Green**. Por consiguiente se estima pertenecer a la clase **Red**.



```{r, echo = TRUE}
library(class)
point <- c(0,0,0)
n <- 6 

model <- knn(train=db[, -4], test=point, cl=db[,4], k = 3) 
kable(model, col.names = c("Predicción"))
```



\subsection{d) Frontera de decisión de Bayes}

Si la frontera de decisión de Bayes en este problema es altamente no
lineal, ¿esperaríamos que el mejor valor de K fuera grande o
pequeño? ¿Por qué?



Cuando K empieza a crecer el modelo empieza a perder flexibilidad tomando una forma lineal. Con un k pequeño el modelo es mas flexible. Con esto en mente el mejor valor para k es cuando k toma un valor pequeño. 

\section{Ejercicio4}
 

\subsection{a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data}

Inicialmente cargamos los datos de la siguiente manera:

```{r, echo = TRUE, eval = FALSE}
library(ISLR)
College=ISLR::College
kable(head(College))
```

```{r,out.width="15%",}
library(ISLR)
College=ISLR::College
College %>% select(Private, Apps, Enroll,Top10perc,Top25perc) %>% head() %>% kable()
```


\subsection{b) Look at the data using the fix() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:}

Para esta sección, mostramos la estructura de los datos, la cual, a groso modo cuenta con datos de 777 universidades 

```{r, echo = TRUE, eval = FALSE}
fix(College)
rownames(College)=College[,1]
College=College [,-1]
fix(College)
```

\subsection{I) Use the summary() function to produce a numerical summary of the variables in the data set.}

Luego, calculamos el resumen estadístico general para todas las variables de la base de datos, este resumen es muy importante, dado que nos da una mejor visión sobre la estructura y el comportamiento de nuestra información.

```{r}
summary(College)
```




Por ejemplo, en promedio el numero de estudiantes matriculados son 780 por universidad, el costo promedio de los libros es aproximadamente 549.4 dólares, la cantidad promedio de empleados para cada universidad es de 13441 personas, otros datos interesantes como: La razón promedio de graduación que es de 65.46, indica que de cada 100 estudiantes aproximadamente 66 se gradúan.

\subsection{II) Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10].}

En esta sección realizamos un diagrama de dispersión, con el fin de observar el grado de asociación lineal entre 
las primeras 10 variables cuantitativas, el resultado fue el siguiente:
  
```{r}
pairs(College[,1:10])
```
Inicialmente observamos una alta asociación entre las variables número de aplicaciones recibidas(apps) y numero de aplicaciones aceptadas(accept), esto indica que en general, que a medida que aumenta la recepción de aplicaciones aumenta también su aceptación, lo cual tiene mucho sentido. De la misma manera existe una alta asociación entre aplicaciones aceptadas(accept) y 
número de estudiantes matriculados(enroll). Otra relación bastante fuerte es entre los estudiantes nuevos que hacen parte del 10% y 25% superior de secundaria, indica que hacer parte de estos porcentajes puede incrementar la probabilidad e asistir a una universidad.

\subsection{III) Use the plot() function to produce side-by-side boxplots of Outstate versus Private.}

Luego, agregamos un gráfico entre universidad publica o privada y el numero de matriculas fuera del estado.

```{r}
plot(College$Outstate~as.factor(College$Private),
     main="Privados vs Matricula Fuera del Estado",xlab='Privado',
     ylab='Matricula fuera del estado')
```
El gráfico anterior nos muestra que aparentemente no existe una diferencia significativa sobre el comportamiento medio para el numero de matrículas fuera del estado para universidades públicas o privadas, aunque se podría pensar que es un poco mayor para las privadas, pero el traslape de las cajas no es muy evidente.

\subsection{IV) Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10 percent of their high school classes exceeds 50 percent.}

En esta sección realizamos una agrupación en una nueva variable categórica llamada Elite, donde los estudiantes con un rendimiento superior del 10 % en secundaria, se agrupan en si, siempre y cuando la universidad contenga 50 o mas de ellos, en caso contrario se agrupan en no, el resultado fue el siguiente:

```{r}
Elite=rep("No",nrow(College ))
Elite[College$Top10perc >50]=" Yes"
Elite=as.factor(Elite)
College=data.frame(College , Elite)
kable(summary(College$Elite), col.names = "Cantidad")
```

```{r}
plot(College$Outstate~as.factor(College$Elite),
     main="Elite vs Matricula Fuera del Estado",xlab='Elite',
     ylab='Matricula fuera del estado')
```


Gráficamente parece haber mayor cantidad de matrículas fuera del estado para los estudiantes elite, pero esta diferencia
no es muy clara, pero da indicios muy fuertes.

\subsection{V) Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative vari ables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.}

Los siguientes histogramas muestran la distribución de algunas de las variables, sin embargo, no es posible concluir acerca de una posible distribución.

```{r, width = 100, height = 100}
par(mfrow=c(2,2))
hist(College$Apps)
hist(College$Accept)
hist(College$Enroll)
hist(College$Top10perc)
```


\subsection{VI) Continue exploring the data, and provide a brief summary of what you discover.}
Continúe explorando los datos.

```{r, warning=FALSE, message=FALSE}
base<-College
base<-base[,2:7]

library(ggplot2)
library(GGally)

gg2<-ggpairs(base,
             upper=list(continuous = wrap("smooth",alpha = 0.3, size=1.2,
                                               method = "lm")),
             lower=list(continuous ="cor"))

for(i in 1:ncol(base)){
  gg2[i,i]<-gg2[i,i]+
    geom_histogram(breaks=hist(base[,i],breaks = "FD",plot=F)$breaks,
                   
                   colour = "red",fill="lightgoldenrod1")
  
}

gg2
ggpairs(base,diag=list(continuous=wrap("box_no_facet",color="red",
                                       fill="lightgoldenrod1",alpha=0.3)),
        upper=list(continuous = wrap("smooth",alpha = 0.3, size=1.2,method = "lm")),
        lower=list(continuous ="cor"))
```



El siguiente gráfico representa el comportamiento de las primeras 8 variables cuan-titativas, reuniendo un conjunto de gráficos muy importantes a la hora de hacer un análisis descriptivo como lo es observar la correlación entre las variables, de esta manera ver si existe relación lineal entre dichas variables.




