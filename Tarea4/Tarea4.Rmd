---
header-includes:
- \usepackage{longtable}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}\decimalpoint
- \setlength{\parindent}{1.25cm}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{float}
- \usepackage{multirow}
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: es
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =FALSE, message = FALSE, fig.align = "center", fig.height = 3.5, fig.pos = "H")

```


```{=tex}
\input{DocumentFormat/titlepage.tex}
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
```

```{=tex}
\pagestyle{myheadings}
\setcounter{page}{4}
```



\section{Ejercicio 1}

1. Este ejercicio utiliza el conjunto de datos OJ el cual es parte de la librería ISLR


\subsection{a) Cree un conjunto de entrenamiento con una muestra aletoria de 800 observaciones y un conjunto de prueba que conste del resto de observaciones.}

Se procede a cargar las librerías necesarias del **R** y a crear un conjunto de entrenamiento de **800** datos para prueba y **270** datos para entrenamiento fijando una semilla = **1** la cual permitirá la replicabilidad de nuestro informe.  

```{r, echo = TRUE}
require(ISLR)
require(tidyverse)
require(ggthemes)
require(caret)
require(e1071)
require(kableExtra)

set.seed(1)

data('OJ')

inTrain <- sample(nrow(OJ), 800, replace = FALSE)

training <- OJ[inTrain,]
testing <- OJ[-inTrain,]
```
\subsection{b) Ajuste un clasificador de soporte vectorial utilizando cost = 0.1, con Purchase como la variable respuesta y las demás como predictores.}


### Utilice la función summary() para obtener un resumen de estadísticas y describa los resultados obtenidos.

Tomando como variable respuesta **Purchase**. Se ajusta un clasificador de soporte vectorial lineal **(SVM Linear)** con un parámetro de **cost = 0.1**

```{r}
svm_linear <- svm(Purchase ~ ., data = training,
                  kernel = 'linear',
                  cost = 0.1)
summary(svm_linear)

```

Este clasificador **SVM** de kernel **lineal** ha sido utilizado con **cost=0.1**, y se obtienen **342** vectores
de soporte, **171** en una clase y **171** en la otra.

\subsection{c) Que tasas de error de entrenamiento y de prueba obtiene?.}

```{r}
postResample(predict(svm_linear, training), training$Purchase)
```
Se obtiene una tasa de precisión del **83.5%** para el conjunto de entrenamiento.

```{r}
postResample(predict(svm_linear, testing), testing$Purchase)
```

Se obtiene una tasa de precisión del **83.7%** para el conjunto de prueba. 


Estos nos indica que este clasificador de soporte vectorial de kernel lineal tiene una alta capacidad predictiva. 


\subsection{d)Utilice la función tune() para obtener un valor óptimo del parámetro cost. Considere valores en el rango de 0.01 a 10.}

```{r}
set.seed(1)
svm_linear_tune <- train(Purchase ~ ., data = training,
                         method = 'svmLinear2',
                         trControl = trainControl(method = 'cv', number = 10),
                         preProcess = c('center', 'scale'),
                         tuneGrid = expand.grid(cost = seq(0.01, 10, length.out = 20)))
```


```{r}
svm_linear_tune
```


\subsection{e) Calcule nuevamente las tasas de error de entrenamiento y de prueba usando el valor  óptimo obtenido de cost.}

```{r}
postResample(predict(svm_linear_tune, training), training$Purchase)
```


```{r}
postResample(predict(svm_linear_tune, testing), testing$Purchase)
```

### Repita items de (b) hasta (e) ajustando esta vez una máquina de soporte vectorial (svm) con un nucle radial. Utilizando el valor de default paray

```{r}
svm_radial <- svm(Purchase ~ ., data = training,
                  method = 'radial',
                  cost = 0.1)
summary(svm_radial)
```


```{r}
postResample(predict(svm_radial, training), training$Purchase)
```


```{r}
postResample(predict(svm_radial, testing), testing$Purchase)
```


```{r}
svm_radial_tune <- train(Purchase ~ ., data = training,
                         method = 'svmRadial',
                         trControl = trainControl(method = 'cv', number = 10),
                         preProcess = c('center', 'scale'),
                         tuneGrid = expand.grid(C = seq(0.01, 10, length.out = 20),
                                                sigma = 0.05))
```


```{r}
svm_radial_tune
```


```{r}
postResample(predict(svm_radial_tune, training), training$Purchase)
```

```{r}
postResample(predict(svm_radial_tune, testing), testing$Purchase)
```


### Repita items (b) hasta (e) utilizando nuevamente una máquina de soporte vectorial pero esta vez con un nucleo polinomial, usando degree = 2. 

```{r}
svm_poly <- svm(Purchase ~ ., data = training,
                  method = 'polynomial', degree = 2,
                  cost = 0.01)
summary(svm_poly)
```


```{r}
postResample(predict(svm_poly, training), training$Purchase)
```


```{r}
postResample(predict(svm_poly, testing), testing$Purchase)
```


```{r}
svm_poly_tune <- train(Purchase ~ ., data = training,
                         method = 'svmPoly',
                         trControl = trainControl(method = 'cv', number = 10),
                         preProcess = c('center', 'scale'),
                         tuneGrid = expand.grid(degree = 2,
                                         C = seq(0.01, 10, length.out = 20),
                                         scale = TRUE))
```


```{r}
svm_poly_tune
```


```{r}
postResample(predict(svm_poly_tune, training), training$Purchase)
```


```{r}
postResample(predict(svm_poly_tune, testing), testing$Purchase)
```


\subsection{h) En general cúal método parece proporcionar los mejores resultados en estos datos?.}

En general, los modelos son muy similares, pero los núcleo lineal y radial funcionan mejor por un pequeño margen.








\section{Ejercicio 2}

```{r, warning=FALSE,message=FALSE}
# Cargo de librerias
library(corrplot)
```


Se considera el conjunto de datos __USArrests__. En este ejercicio se agruparán los
estados en __USArrests__ con agrupamiento jerarquico. Este conjunto de datos contiene estadísticas, en arrestos por cada 100,000 residentes por agresión, asesinato y violación en cada uno de los 50 estados de EE. UU. en 1973. También se proporciona el porcentaje de la población que vive en áreas urbanas.

Primeramente, se procede a cargar la base de datos __USArrests__  y examinar sus características:

```{r}
head(USArrests) # encabezado de la base
str(USArrests) # caracteristicas de la base
```

Se observa como la base cuenta con 50 observaciones y 4 variables las cuales todas son numericas y su descripción se presenta seguidamente:

- __Murder:__ Arrestos por asesinato (por 100.000).
- __Assault:__  Arrestos por asalto (por 100.000).
- __UrbanPop:__ Porcentaje de población urbana.
- __Rape:__  Arrestos por violaciones (por 100.000).

Adicionalmente, se presentan un análisis descriptivos de estas variables:

```{r, fig.height= 3, fig.width= 7}
pairs(USArrests)
```

Del anterior gráfico de dispersión entre las variables, se observa como entre cada par de combinación de variables, existe una relación creciente. Lo cual en primera instancia podría ser un indicativo de que posiblemente en los estados donde se presente mayor porcentaje de población urbana también se puede presentar mayores casos de arrestos por asalto, asesinato o violación.

Por otro lado, se presenta una matriz de correlación entre las cuatro variables:

```{r}
cor(USArrests)
```

También, se presenta un gráfico de correlaciones de estas variables:

```{r, fig.height= 3, fig.width= 7}
# se crea la matriz de correlación
corr.data <- cor(USArrests)
# gráfico de correlacion para las variables cuantitativas
corrplot(corr.data, method = 'ellipse', order='AOE', type = 'upper')

```

De los resultados obtenidso anteriormente, se observa como:

- Existe una alta correlación positiva entre los arrestos por asesinato y los arrestos por asaltos, la cual es de un 0.8018733, Esto puede indicar que, así como pueden aumentar los arrestos por asalto en un estado de USA, también puede aumentar los arrestos por asesinato en ese mismo estado.

- Existe una alta correlación positiva entre los arrestos por asalto y los arrestos por violaciones, la cual es de un 0.6652412, Esto puede indicar que, así como pueden aumentar los arrestos por asalto en un estado de USA, también puede aumentar los arrestos por violaciones en ese mismo estado.

- Se observa en la matriz de correlaciones como, no existe una aparente correlación significativa entre los arrestos por asesinatos y el porcentaje de población urbana.

## a)

Se utiliza agrupación jerárquica con enlace completo y distancia euclidiana, para agrupar los estados, de la siguiente forma:

```{r}
# ajustando un enlace completo con la distancia euclidiana
hc_complete =hclust(dist(USArrests, method = "euclidean"), method ="complete")
```

Luego se presenta el __dendrograma__ de dicho enlace completo:

```{r, fig.height= 4.5, fig.width= 8}
plot(hc_complete ,main =" Complete Linkage ", xlab="", sub ="",
cex =.7, col = "Purple")
```

## b)

Se procede a separar en el __dendograma__ a una altura que dé como resultado 3 _clusters_.

```{r, fig.height= 4.5, fig.width= 8}
plot(hc_complete ,main =" Complete Linkage ", xlab="", sub ="",
cex =.7, col = "Purple")
rect.hclust(hc_complete, k=3, border=2:10)

```
Luego, usando la función __cutree()__ se puede observar las etiquetas a las que pertenece cada estado según el cluster al que se le asigno.

```{r}
cutree (hc_complete, 3)
```

## c)

Ahora se procede a escalar las variables a fin de tener una desviación estándar de uno y luego se realiza la respectiva agrupación jerárquica usando un enlace completo y la distancia euclidiana.

La función __scale()__ permite escalar las variables, así como el proceso de agrupación jerárquica se muestra a continuación:

```{r}
# ajustando un enlace completo con la distancia euclidiana y las variables escaladas
hc_complete_scale =hclust(dist(scale(USArrests), method = "euclidean"), 
                           method ="complete")
```

Luego se presenta el __dendrograma__ de dicho enlace completo:

```{r, fig.height= 5, fig.width= 10}
plot(hc_complete_scale ,main ="Hierarchical Clustering with Scaled Features", 
     xlab="", sub ="",
cex =.7, col = "dark blue")
```

## d)

Se observa como, en el __dendrograma__ correspondiente a las agrupaciones jerárquicas con las variables escaladas, es notablemente distinto a la agrupación jerárquica generada sin las variables escaladas. Dado que, si bien estamos tratando con los mismos datos, la escalación de variables hace que en cada sub rama existan agrupaciones más uniformes. Inicialmente con las variables sin escalar se observaba claramente una distinción entre tres grupos o _clusters_ distintos. En cambio, realizando el escalado de las variables se puede observar una posible distinción entre 4 _clusters_. 

A continuación se presenta una posible agrupación entre 4 _clusters_:

```{r, fig.height= 5, fig.width= 10}
plot(hc_complete_scale ,main ="Hierarchical Clustering with Scaled Features", 
     xlab="", sub ="",
cex =.7, col = "dark blue")
rect.hclust(hc_complete_scale, k=4, border=2:10)
```

Aunque también podría ser una agrupación de tres _clusters_.

En definitiva, se considera que las variables deben ser escaladas previamente, ya que proporciona una mejor estabilidad a la hora de hacer agrupaciones jerárquicas. Porque es bien sabido que la distancia euclidiana no tiene en cuenta el tipo de escala en la cual se encuentran las variables. Lo cual hace que, en ocasiones, usar esta medida no sea del todo preciso y como se pudo observar en los literales anteriores, se obtuvieron _clusters_ y __dendrogramas__ distintos.



