---
header-includes:
- \usepackage{longtable}
- \usepackage[utf8]{inputenc}
- \usepackage[spanish]{babel}\decimalpoint
- \setlength{\parindent}{1.25cm}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{cancel}
- \usepackage{array}
- \usepackage{float}
- \usepackage{multirow}
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
fontsize: 12pt
papersize: letter
geometry: margin = 1in
language: es
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning =FALSE, message = FALSE, fig.align = "center", fig.height = 3.5, fig.pos = "H")

library(ISLR2)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(data.table)
library(boot)
library(splines)
library(randomForest)
library(tree)
library (MASS)
```


```{=tex}
\input{DocumentFormat/titlepage.tex}
\thispagestyle{empty}
\tableofcontents
\newpage
\thispagestyle{empty}
```

```{=tex}
\pagestyle{myheadings}
\setcounter{page}{4}
```



\section{Ejercicio1}

\subsection{a) Use la función poly() para ajustar una regresión polinomial cúbica y
con esta predecir la variable nox usando dis. Reporte el resultado de la
regresión, luego grafique los datos resultantes y los ajustes polinómicos.}


```{r, echo=TRUE}
attach(Boston)
```

```{r, echo=TRUE}
fit1 <- lm(nox ~ poly(dis,4), data = Boston)
coef(summary(fit1))
```
Al ajustar la regresión correspondiente, se pudo determinar que es suficiente considerar un polinomio de grado 3. Mientras que con un polinomio de grado 4, se comienzan a presentar problemas de sobreparametrización y, por lo tanto, algunos de los parámetros dejan de ser significativos para el modelo.

```{r, echo=TRUE}
fit <- lm(nox ~ poly(dis,3), data = Boston)
(summary(fit))
```
Todas las variables en este modelo de regresión cúbica parecen tener valores p que son estadísticamente significativos. El error estándar residual es bajo pero los grados de libertad son altos. Los valores de R-Squared son relativamente altos en 0.71 para explicar la varianza en el modelo.



```{r, echo=TRUE}
dislims <- range(dis)
dis.grid <- seq(from = dislims[1], to = dislims [2])
preds <- predict(fit , newdata = list(dis = dis.grid),se = TRUE)
se.bands <- cbind(preds$fit + 2 * preds$se.fit ,preds$fit - 2 * preds$se.fit)
```

```{r, echo=TRUE}
par(mfrow = c(1, 1), mar = c(4.5 , 4.5, 1, 1),
oma = c(0, 0, 4, 0))
plot(dis , nox , xlim = dislims , cex = .5, col = "darkgrey")
title("Degree 3 Polynomial", outer = T)
lines(dis.grid, preds$fit , lwd = 2, col = "blue")
matlines(dis.grid , se.bands, lwd = 1, col = "blue", lty = 3)

```

Graficamente, observamos un ajuste bastante bueno al considerar un polinomio de grado 3.


\subsection{b) Grafique los ajustes polinómicos para un rango de polinomios de diferentes grados (digamos, de 1 a 10), y reporte la suma de cuadrados de los residuales asociada.}

Se ajustarán modelos que van desde uno lineal hasta un polinomio de grado 5, para determinar el modelo más simple que sea suficiente para explicar la relación entre nox y dis.


```{r}
fit.1 <- lm(nox ~ dis , data = Boston)
fit.2 <- lm(nox ~ poly(dis , 2), data = Boston)
fit.3 <- lm(nox ~ poly(dis , 3), data = Boston)
fit.4 <- lm(nox ~ poly(dis , 4), data = Boston)
fit.5 <- lm(nox ~ poly(dis , 5), data = Boston)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
coef(summary(fit.5))

rss <- rep(NA, 10)
for (i in 1:10) {
  poly.fit <- lm(nox ~ poly(dis, i), data=Boston)
  rss[i] <- sum(poly.fit$residuals^2)
}
```

Parece ser más apropiado utilizar el modelo cúbico donde El p-valor que compara el polinomio de grado 4, Model3 y Model4, es aproximadamente 59% mientras que el polinomio de grado 5 Model5 parece innecesario por qué su p-valor es 0.03. Por lo tanto, un modelo cuadrático parece proporcionar un ajuste razonable a los datos en comparación a modelos de grados inferiores y superiores. 



```{r, echo=TRUE}
#library(data.table)
rss <- data.table(seq(1:10), rss, keep.rownames = TRUE)
ggplot(rss, aes(V1, rss)) + 
  geom_line() + 
  scale_x_continuous(breaks=c(1:10)) +
  labs(x='Degree of Polynomial', y='Residual Sum of Squares', title='Degree of Polynomial vs RSS')

```


```{r, echo=TRUE}
rss[c(2,3),]

```

el menor Rss se obtiene con 10 grados de libertad.


\subsection{c) Realice una validación cruzada o algún otro enfoque para seleccionar el óptimo grado para el polinomio y explique sus resultados.}


### K-fold Cross-Validation


```{r, echo=TRUE}
#library(boot)
#library(ggplot2)
cv_MSE_k10 <- rep(NA,10)

for (i in 1:10) {
  modelo <- glm(nox ~ poly(dis, i), data = Boston)
  set.seed(17)
  cv_MSE_k10[i] <- cv.glm(data = Boston, glmfit = modelo, K = 10)$delta[1]
}
ggplot(data = data.frame(polinomio = 1:10, cv_MSE = cv_MSE_k10),
       aes(x = polinomio, y = cv_MSE)) +
geom_point(colour = c("firebrick3")) +
geom_path() +
scale_x_continuous(breaks = c(0:10)) +
theme_bw() + 
labs(title  =  'Test Error ~ Grado del polinomio') +
theme(plot.title = element_text(hjust = 0.5, face = 'bold'))
```

Tras el proceso de validación cruzada, se determina que el mejor grado del polinomio es efectivamente el de grado 3, dado que es donde se presenta un menor MSE. 

\subsection{d) Use la función bs() para ajustar una spline de regresión para predecir nox usando dis. Reporte la salida para el ajuste usando cuatro grados de libertad. ¿Cómo ubicó los nodos?. Grafique el ajuste resultante.}


```{r, echo=TRUE}
#library(splines)
fit <- lm(nox ~ bs(dis , knots = c(3.2)), data = Boston)
pred <- predict(fit , newdata = list(dis = dis.grid), se = T)
plot(dis , nox , col = "gray")
lines(dis.grid, pred$fit , lwd = 2)
lines(dis.grid , pred$fit + 2 * pred$se, lty = "dashed")
lines(dis.grid , pred$fit - 2 * pred$se, lty = "dashed")

```

```{r, echo=TRUE}
#library(ggplot2)

attr(bs(dis , df = 4), "knots")

```
Para ajustar una spline de regresión con 4 grados de libertad, se utilizó la función attr la cual nos permite determinar la posición de los nodos según los grados de libertad, en este caso, como acabamos de observar, para 4 grados de libertad, solo permite un nodo que representa la mediana en 3.2.


\subsection{e) Ahora ajuste una spline de regresión para un rango de grados de libertad, y grafique los ajustes resultantes e informe el RSS resultante. Describa los resultados obtenidos.}


```{r, echo=TRUE}
plot.new()
plot(dis , nox , xlim = dislims , cex = .5, col = "darkgrey")
title("Spline regression")
# 4grados de libertad
fit.1 <- lm(nox ~ bs(dis , knots = c(3.2)), data = Boston)
# 6grados de libertad
attr(bs(dis , df = 6), "knots")
fit.2 <- lm(nox ~ bs(dis , knots = c(2.1,3.2,5.18)), data = Boston)
# 7grados de libertad
attr(bs(dis , df = 7), "knots")
fit.3 <- lm(nox ~ bs(dis , knots = c(1.95,2.6,3.87,5.61)), data = Boston,cv=T)
#spline suave, con grados de libertad obtenidos a partir de cv
fit.4 <- smooth.spline(dis , nox , cv = TRUE)

pred1 <- predict(fit.1 , newdata = list(dis = dis.grid), se = T)
pred2 <- predict(fit.2, newdata = list(dis = dis.grid), se = T)
pred3<- predict(fit.3, newdata = list(dis = dis.grid), se = T)

lines(dis.grid, pred1$fit ,col='red', lwd = 2)
lines(dis.grid, pred2$fit ,col='blue', lwd = 2)
lines(dis.grid, pred3$fit ,col='green', lwd = 2)
lines(fit.4 , col = "yellow", lwd = 2)

legend("topright", legend = c("4 DF", "6 DF",'8 DF','15.4 DF'),col = c("red", "blue",'green','yellow'), lty = 1, lwd = 2, cex = .8)

```


Se observan splines de regresión con 4, 6, 7 y un spline suave con 15.4 gl, claramente el mejor ajuste se observa para un spline cúbico con 4 grados de libertad, a medida que se aumentan los grados de libertad, la tendencia del ajuste se deja llevar por el ruido y esto genera problemas de predicción.


### Analisis de RSS


```{r, echo=TRUE}

rss <- rep(NA, 10)
for (i in 1:10) {
  poly.fit <- lm(nox ~ poly(dis, i), data=Boston)
  rss[i] <- sum(poly.fit$residuals^2)
}

plot(1:10, rss, xlab='Degree of Polynomial', ylab='Residual Sum of Squares', type='b', main='Degree of Polynomial vs RSS')
axis(1, at = seq(1,10, by=1))



```



\subsection{f) Realice una validación cruzada o algún otro enfoque para seleccionar los mejores grados de libertad para una spline de regresión sobre estos datos. Describa sus resultados.}



```{r, echo=TRUE}
#library(boot)
#library(data.table)
#library(ggplot2)
cv.spline.fun <- function(i) {
  fit <- glm(nox ~ bs(dis, df=i), data=Boston)
  cv.error <- cv.glm(Boston, fit, K=10)$delta[2]
}

cv.err <- sapply(1:20, cv.spline.fun)

df <- seq(1:20)

dt <- data.table(df, cv.err)

#plot.new()
ggplot(dt, aes(df, cv.err)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks=seq(1:20)) +
  labs(x='Degrees of Freedom', y='CV MSE', title='Spline - CV MSE vs Degrees of Freedom')
```


```{r, echo=TRUE}
#library(gridExtra)

p1 <- ggplot(Boston, aes(x=dis, y=nox)) +
    geom_point(colour='black') +
    stat_smooth(method='lm', formula= y ~ bs(x,df=3)) +
  labs(x='dis',y='nox', title='Degrees of Freedom 3')

p2 <- ggplot(Boston, aes(x=dis, y=nox)) +
    geom_point(colour='black') +
    stat_smooth(method='lm', formula= y ~ bs(x,df=5)) +
  labs(x='dis',y='nox', title='Degrees of Freedom 5')


p3 <- ggplot(Boston, aes(x=dis, y=nox)) +
    geom_point(colour='black') +
    stat_smooth(method='lm', formula= y ~ bs(x,df=10)) +
  labs(x='dis',y='nox', title='Degrees of Freedom 10')


p4 <- ggplot(Boston, aes(x=dis, y=nox)) +
    geom_point(colour='black') +
    stat_smooth(method='lm', formula= y ~ bs(x,df=13)) +
  labs(x='dis',y='nox', title='Degrees of Freedom 13')


grid.arrange(p1,p2,p3,p4, ncol=2, nrow=2)
```


Se observa que el ajuste puede darse entre 5 y 10 grados de libertad, aunque según el RSS el que mejor ajuste tiene es el de 10 grados de libertad. 

```{r}
rm(list = ls())
```


\section{Ejercicio2}

En este ejercicio se utilizarán arboles de regresión para predecir los valores de
la variable __sales__ en la base de datos __Carseats__ de la libreria __ISLR2__, tratando
dicha variable como continua:

Primeramente, se cargan los datos y se examinan sus caracteristicas:

```{r, echo = TRUE}
head(Carseats)
str(Carseats)
```

La base __Carseats__ constituye un conjunto de datos simulados que contiene las ventas de sillas de coche para niños en 400 tiendas diferentes.

Dicha base contiene las siguientes 11 variables:

- __Sales:__ Ventas unitarias (en miles) en cada ubicación.

- __CompPrice:__ Precio cobrado por el competidor en cada ubicación.

- __Income:__ Nivel de ingresos de la comunidad (en miles de dólares).

- __Advertising:__ Presupuesto de publicidad local para la empresa en cada ubicación (en miles de dólares).

- __Population:__ Tamaño de la población en la región (en miles).

- __Price:__ Precio de los cargos de la compañía por los asientos de seguridad en cada sitio.

- __ShelveLoc:__ Un factor con niveles en el que __Bad__ indica la calidad de la ubicación de las estanterías para los asientos de automóvil en cada __sitioGoodMedium__.

- __Age:__ Edad media de la población local.

- __Education:__ Nivel de educación en cada lugar.

- __Urban:__ Un factor con niveles __No__ y __Yes__ para indicar si la tienda está en una zona urbana o rural.

- __US:__ Un factor con niveles __No__ y __Yes__ para indicar si la tienda está en USA o no.


\subsection{a)Conjunto de entrenamiento y prueba}

Se procede a dividir el conjunto de observaciones en un conjunto de entrenamiento y un
conjunto de prueba. De la siguiente manera:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
# cargo de libreria para arboles de regresión
library(tree)
```

De forma aleatoria se realiza un _sample_ del conjunto de datos para posteriormente crear los conjuntos de prueba y de entrenamiento:

```{r, echo=TRUE}
set.seed (123)
train = sample(1: nrow(Carseats), nrow(Carseats)*0.7)
```
### Conjunto de entrenamiento

```{r, echo=TRUE}
# datos de entrenamiento para la base Carseats
train_car = Carseats[train, ]
dim(train_car) # dimensiones de los datos de entrenamiento
head(train_car) # encabezado de los datos de entrenamiento
```

### Conjunto de prueba

```{r, echo=TRUE}
# datos de prueba para la base Carseats
test_car = Carseats[-train, ]
dim(test_car) # dimensiones de los datos de prueba
head(test_car) # encabezado de los datos de prueba
```

Los datos de entrenamiento y prueba se dividieron en proporciones de 0.7 y 0.3 con respecto a la base __Carseats__. Es decir, el 70% de dicha base corresponden con datos de entrenamiento (280 registros) y el 30% con los datos de prueba (120 registros).

\subsection{b)Arbol de regresión}

Ahora se ajusta un árbol de regressión en el conjunto de entrenamiento. 

```{r, echo=TRUE}
# ajuste del arbol de regresión
tree_carseats <-  tree(Sales~.,Carseats,subset = train)

# resumen del modelo
summary(tree_carseats)
```

Se observa como el modelo de regresión seleccionó 5 variables las cuales son: __"ShelveLoc"__, __"Price"__,  __"CompPrice"__, __"Age"__  y __"Advertising"__. Además, dicho árbol estableció 19 nodos terminales.

Luego, se Gráfica dicho árbol y se interpretan los resultados.

```{r, echo=TRUE, fig.height=8, fig.width=10}
plot(tree_carseats, col = '#1E90FF')
text(tree_carseats, pretty = 0, cex = 0.5)
title('Árbol de Regresión', cex = 2)
```

### Interpretación

Se observa del árbol de regresión como en su nivel más alto, separa por el factor  __ShelveLoc__, en los niveles __bad__ y __medium__. En su segundo nivel más alto separa por la variable __Price__ esto índica que los precios de los cargos de la compañía por los asientos de seguridad en cada sitio. En este caso, cuando __ShelveLoc__ está en un nivel __bad__ son menores a 105.5. Mientras que, cuando __ShelveLoc__ está en un nivel __medium__ son menores a 109.5. También se observa como los precios varían según la edad y el precio cobrado por cada competidor en cada ubicación.

También se observa como, el indicador más importante de __Sales__ parece ser un “nivel de calidad de ubicación” ( __ShelveLoc__ ) debido a que la primera rama separa la categoría __Good__ de las categorías __Bad__ y __Medium__. Cuando el precio de la silla de bebé para carro ( __Price__ ), tiende a ser más alto, las ventas son en promedio menores.

Por otro lado, cuando la calidad de ubicación es buena ( __ShelveLoc__ ), las ventas son en promedio más altas que cuando la calidad de ubicación es mala. No obstante, cuando la inversión de publicidad es más alta ( __Advertising__ ), las ventas son mayores.

__¿Qué valor para el MSE de prueba se obtiene?__

Para dar respuesta a esta pregunta se realiza lo siguiente:

```{r, echo=TRUE}
yhat <- predict(tree_carseats, newdata = test_car)
mean((yhat - test_car$Sales)^2)
```

Se obtiene un error de prueba de 3.602818; la raiz cuadrada de dicho valor es 1.898109; indicando que en este modelo las ventas están alrededor de $1.898109 del verdadero valor.

\subsection{c) Validadación cruzada y poda del arbol de regresión}

Se utiliza la validación cruzada usando la función __cv.tree()__ para ver si una poda del árbol mejora su desempeño y a fin de determinar el grado  óptimo de complejidad del árbol.

```{r, echo=TRUE}
# validación cruzada con el árbol de regresión de entrenamiento
set.seed(123)
cv_carseats =cv.tree(tree_carseats)
plot(cv_carseats$size ,cv_carseats$dev,type='b', col = "#1E90FF",
      ylab = 'Error de validación cruzada', xlab = 'Nodos terminales', 
     main = 'Error de validación vs Nodos')
abline(v=8, col="red", lwd=3, lty=2)
```

Según lo evidencia el gráfico, el grado óptimo de complejidad del árbol, es aquel que cuenta con 8 nodos.

Ahora bien si se quiere podar el árbol, se utiliza la función __prune.tree()__.

```{r, echo=TRUE, fig.height=8, fig.width=10}
# Se poda el arbol de regresión
prune_carseats = prune.tree(tree_carseats ,best = 8)
plot(prune_carseats, col = "#CD1076")
text(prune_carseats ,pretty =0, cex = 0.7)
title('Árbol de Regresión', cex = 2)
```

### Interpretación

De este arbol de regresion se observa como, cuando los ingresos de la comunidad ( __income__ ) son menores, las ventas son menores. El indicador más importante de __Sales__ parece ser un “nivel de calidad de ubicación” ( __ShelveLoc__ ) debido a que la primera rama separa la categoría __Good__ de las categorías __Bad__ y __Medium__.

Además, cuando el precio de la silla de bebé para carro (__Price__), tiende a ser más alto, las ventas son en promedio menores. Cuando la calidad de ubicación es bueno ( __ShelveLoc__ ), las ventas son en promedio más altas que cuando la calidad de ubicación es mala. Por otro lado, cuando la inversión de publicidad es más alta ( __Advertising__ ), las ventas son mayores.

__¿Qué valor para el MSE de prueba se obtiene?__

Para dar respuesta a esta pregunta se realiza lo siguiente:

```{r, echo=TRUE}
yhat_cv <- predict(prune_carseats, newdata = test_car)
mean((yhat_cv - test_car$Sales)^2)
```

Se observa como a pesar de que se disminuyo la complejidad de los nodos terminales, el MSE obtenido aumento, en este caso paso de ser de 3.602818 a 4.353022 con la poda. Por lo cual en este caso la poda del árbol no consiguió mejorar el MSE de prueba.

\subsection{d) Bagging}

Se utiliza el método __bagging__ para analizar estos datos __Carseats__ con la función __randomForest()__  y usando la función __importance()__ para determinar cuál de las variables es la más importante. Dicho procedimiento se muestra como sigue:

```{r, echo=TRUE, warning=FALSE,message=FALSE}
# cargando el paquete para realizar Random Forest y Bagging
library(randomForest) 
set.seed(1)
bag_carseats = randomForest(Sales~.,data=Carseats ,subset =train,mtry=10, importance =TRUE)
bag_carseats
```

Se utilizan 500 árboles de regresión, para ajustar un modelo bagging. Ahora se el MSE de prueba con el método __bagging__:

```{r, echo=TRUE}
# MSe con el método bagging
yhat.bag <- predict(bag_carseats, newdata = test_car)
mean((yhat.bag - test_car$Sales)^2)
```
- Se observa que con el método __bagging__ se obtiene un MSE de 2.272119, menor al de los árboles anteriores con y sin la poda.

Utilizando la función “importance()“, se indaga por la importancia de cada variable:

```{r, echo=TRUE}
importance(bag_carseats)
```

- Cuando las variables __ShelveLoc__ y __Price__ se excluyen, se presenta la mayor disminución media en la precisión de las ventas, en las muestras __bagging__, estas medidas son de 50.78 y 46.38, respectivamente.

- Cuando las variables __ShelveLoc__ y __Price__ se incluyen, se presenta el mayor incremento total de la pureza del nodo que resulta de divisiones sobre la variable, promediada sobre todos los árboles.


```{r, echo=TRUE}
varImpPlot(bag_carseats, col = '#43CD80', main = 'Modelo bagged')
```

Los resultados indican que en todos los árboles considerados en el __bagging__, la calidad de ubicación ( __ShelveLoc__ ) y el precio de las silla de bebé( __Price__ ), son las dos variables más importantes.

\subsection{e) Bosque aleatorio}

Ahora se utiliza un bosque aleatorio (Random-Forest) para analizar estos datos Carseats y predecir las ventas.

```{r, echo=TRUE}
set.seed(6270)
rf_car <- randomForest(Sales ~ ., data = train_car, mtry = 4, importance=TRUE)
rf_car
```

Se utilizaron 500 arboles para construir el bosque aleatorio, cada uno con 5 variables. Ahora se el MSE de prueba con el método __random forest__:

```{r, echo=TRUE}
# MSe con el método random forest
yhat.rf<- predict(rf_car, newdata = test_car)
mean((yhat.rf - test_car$Sales)^2)
```

- Se observa que con el método __random forest__ se obtiene un MSE de 2.44811, un poco superior al obtenido con el método __bagging__ pero menor al obtenido de los árboles anteriores con y sin la poda.

Utilizando la función “importance()“, se indaga por la importancia de cada variable:

```{r, echo=TRUE}
importance(rf_car)
```

- Cuando las variables __ShelveLoc__ y __Price__ se excluyen, se presenta la mayor disminución media en la precisión de las ventas, en las muestras __bagging__, estas medidas son de 52.89 y 43.51, respectivamente.

- Cuando las variables __ShelveLoc__ y __Price__ se incluyen, se presenta el mayor incremento total de la pureza del nodo que resulta de divisiones sobre la variable, promediada sobre todos los árboles.


```{r, echo=TRUE}
varImpPlot(bag_carseats, col = '#6A5ACD', main = 'Modelo Random Forest')
```

Los resultados indican que en todos los árboles considerados en el __Random Forest__, la calidad de ubicación ( __ShelveLoc__ ) y el precio de las silla de bebé( __Price__ ), son las dos variables más importantes.

__Describa el efecto de m el número de variables consideradas en cada subdivisión, en la tasa de error obtenida.__

Para la construcción del modelo random forest se utilizó m=4, que corresponde a la función techo de 10/3, y se obtiene un error de prueba muy similar al utilizado con bagged, pero con este se garantiza que hay una menor correlación entre árboles.

Se considera m=5, para ver si este mejora las estimaciones en el conjunto de prueba.

```{r, echo=TRUE}
set.seed(6270)
rf_car2 <- randomForest(Sales ~ ., data = train_car, mtry = 5, importance=TRUE)
rf_car2
```

Luego se cálcula su respectivo MSE de prueba:

```{r, echo=TRUE}
# MSe con el método random forest
yhat.rf2<- predict(rf_car2, newdata = test_car)
mean((yhat.rf2 - test_car$Sales)^2)
```

Cuando se utiliza m=5, la tasa de error de prueba disminuye con respecto a bagged, hay menor correlación entre los árboles debido a que no se consideran todos los predictores y por lo tanto se puede disminuir la varianza.

